{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Set up","metadata":{"id":"G-QND2RyjfoQ"}},{"cell_type":"code","source":"# General import\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nimport math\nfrom random import random\nimport scipy.signal\nimport cv2\nimport os\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.model_selection import GridSearchCV, PredefinedSplit\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\n\n# CNN import \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import datasets, layers, models, Model\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import LearningRateScheduler","metadata":{"id":"2faf5ad5","execution":{"iopub.status.busy":"2021-12-09T02:01:03.290899Z","iopub.execute_input":"2021-12-09T02:01:03.291378Z","iopub.status.idle":"2021-12-09T02:01:10.775811Z","shell.execute_reply.started":"2021-12-09T02:01:03.291296Z","shell.execute_reply":"2021-12-09T02:01:10.774842Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip freeze > requirements.txt  # Python2","metadata":{"id":"xeEjTdsSjPaa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Var Path\ndata_path = '../input/fall2021-inf8245e-machine-learning/'\nsubmit_path = './'","metadata":{"id":"8799efc7","execution":{"iopub.status.busy":"2021-12-09T02:03:11.442404Z","iopub.execute_input":"2021-12-09T02:03:11.443188Z","iopub.status.idle":"2021-12-09T02:03:11.447788Z","shell.execute_reply.started":"2021-12-09T02:03:11.443146Z","shell.execute_reply":"2021-12-09T02:03:11.447014Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Verif if we are have access to a GPU\n# gpu_info = !nvidia-smi\n# gpu_info = '\\n'.join(gpu_info)\n# if gpu_info.find('failed') >= 0:\n#   print('Not connected to a GPU')\n# else:\n#   print(gpu_info)","metadata":{"id":"vsWQ6giPjCuL","execution":{"iopub.status.busy":"2021-12-09T02:01:21.725400Z","iopub.execute_input":"2021-12-09T02:01:21.725841Z","iopub.status.idle":"2021-12-09T02:01:21.756146Z","shell.execute_reply.started":"2021-12-09T02:01:21.725803Z","shell.execute_reply":"2021-12-09T02:01:21.755156Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def fix_seed_random(seed_value):\n    # 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n    import os\n    os.environ['PYTHONHASHSEED']=str(seed_value)\n\n    # 2. Set the `python` built-in pseudo-random generator at a fixed value\n    import random\n    random.seed(seed_value)\n\n    # 3. Set the `numpy` pseudo-random generator at a fixed value\n    import numpy as np\n    np.random.seed(seed_value)\n\n    # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n    import tensorflow as tf\n    tf.random.set_seed(seed_value)\n    tf.compat.v1.set_random_seed(seed_value)\n\n    # 5. Configure a new global `tensorflow` session\n    from keras import backend as K\n    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n    tf.compat.v1.keras.backend.set_session(sess)\n\nfix_seed_random(0)","metadata":{"id":"B9bJHBo5i0TM","execution":{"iopub.status.busy":"2021-12-09T02:01:25.139358Z","iopub.execute_input":"2021-12-09T02:01:25.139681Z","iopub.status.idle":"2021-12-09T02:01:25.791262Z","shell.execute_reply.started":"2021-12-09T02:01:25.139650Z","shell.execute_reply":"2021-12-09T02:01:25.790194Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Import the raw data\nX_raw = pd.read_pickle(data_path + \"x_train.pkl\")\ny_raw = pd.read_pickle(data_path + \"y_train.pkl\")\nx_test_raw = pd.read_pickle(data_path + \"x_test.pkl\")","metadata":{"id":"47bc37b0","execution":{"iopub.status.busy":"2021-12-09T02:03:16.720162Z","iopub.execute_input":"2021-12-09T02:03:16.721005Z","iopub.status.idle":"2021-12-09T02:03:21.615206Z","shell.execute_reply.started":"2021-12-09T02:03:16.720968Z","shell.execute_reply":"2021-12-09T02:03:21.614240Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Data exploration","metadata":{"id":"38c66737"}},{"cell_type":"code","source":"i = int(random() * len(y_raw))\nprint(i)\nimg = X_raw[i]\ny = y_raw[i]\nplt.imshow(img, interpolation='nearest')\nplt.show()\nimg_height, img_width = img.shape\nprint(f\"Image of a {y} with the shape: {img_height}x{img_width}\")","metadata":{"id":"Od7u_iGHzWNA","execution":{"iopub.status.busy":"2021-12-09T02:03:41.623358Z","iopub.execute_input":"2021-12-09T02:03:41.623671Z","iopub.status.idle":"2021-12-09T02:03:41.835244Z","shell.execute_reply.started":"2021-12-09T02:03:41.623636Z","shell.execute_reply":"2021-12-09T02:03:41.834454Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"unique, counts = np.unique(y_raw, return_counts=True)\n\nresult = np.column_stack((unique, counts)) \nnum_class = len(result)\nprint(f\"The dataset have {num_class} class\")\nprint(\"Number of occ of each class in your data set\")\nfor r in result:\n  print(f\"{r[0]}: {r[1]} pictures\")","metadata":{"id":"69c5f095","execution":{"iopub.status.busy":"2021-12-09T02:04:13.501315Z","iopub.execute_input":"2021-12-09T02:04:13.501604Z","iopub.status.idle":"2021-12-09T02:04:13.514637Z","shell.execute_reply.started":"2021-12-09T02:04:13.501573Z","shell.execute_reply":"2021-12-09T02:04:13.513546Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Filter ","metadata":{"id":"q3VAqQ-Ys3fE"}},{"cell_type":"code","source":"def mean_filter(img):\n    Mean_filter = np.array([[1,1,1], [1,1,1], [1,1,1]])/float(9)\n    return scipy.signal.convolve2d(img,Mean_filter,mode='same')\n\ndef median_filter(img):\n    # img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)    \n    Mean_filter = np.array([[1,1,1], [1,1,1], [1,1,1]])/float(9)\n    return cv2.medianBlur(img, 3)\n\ndef gaussian_filter(img):\n    \"\"\"\n    return: ['Original', 'Filtered', 'High Components', 'Enhanced']\n    \"\"\"\n    Hg = np.zeros((20,20))\n    # gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    for i in range(20):\n        for j in range(20):\n            Hg[i,j] = np.exp(-((i-10) ** 2 + (j-10)**2)/10)\n\n    gaussian_blur = scipy.signal.convolve2d(img, Hg, mode='same')\n    gray_high = img - gaussian_blur\n    gray_enhanced = img + 0.025 * gray_high\n\n    column = 2\n    row = 2\n\n    return np.array([gaussian_blur, gray_high, gray_enhanced])\n\ndef edge_filter(img):\n    # gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    Hx = np.array([[1,0,-1], [2,0,-2],[1,0,-1]], dtype=np.float32)\n    Hy = np.array([[-1,-2,-1],[0,0,0],[1,2,1]], dtype=np.float32)\n    Gx = scipy.signal.convolve2d(img, Hx, mode ='same')\n    Gy = scipy.signal.convolve2d(img, Hy, mode = 'same')\n    G = (Gx*Gx + Gy*Gy) ** 0.5\n    return np.array([Gx, Gy, G])","metadata":{"id":"LF_OMok8zY8a","execution":{"iopub.status.busy":"2021-12-09T02:04:15.634582Z","iopub.execute_input":"2021-12-09T02:04:15.634976Z","iopub.status.idle":"2021-12-09T02:04:15.650414Z","shell.execute_reply.started":"2021-12-09T02:04:15.634917Z","shell.execute_reply":"2021-12-09T02:04:15.649310Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\n\n# Get Random Image\n# i = int(random() * len(y_raw))\ni = 259\nimg = X_raw[i]\n\n# Mean filter\nfiltered_img = mean_filter(img)\nplt.figure()\nf, axarr = plt.subplots(1,2) \nf.suptitle(\"Mean filter\", fontsize=14)\naxarr[0].imshow(img) \naxarr[0].title.set_text('Original')\naxarr[1].imshow(filtered_img)\naxarr[1].title.set_text('Mean filter')\n\n# Median filter\nfiltered_img = median_filter(img)\nplt.figure()\nf, axarr = plt.subplots(1,2) \nf.suptitle(\"Median filter\", fontsize=14)\naxarr[0].imshow(img)\naxarr[0].title.set_text('Original')\naxarr[1].imshow(filtered_img)\naxarr[1].title.set_text('Median filter')\n\n# Gaussian filter\nimgs = gaussian_filter(img)\nplt.figure()\nf, axarr = plt.subplots(1,3) \nf.suptitle(\"Gaussian filter\", fontsize=14)\naxarr[0].imshow(imgs[0])\naxarr[0].title.set_text('Filtered')\naxarr[1].imshow(imgs[1])\naxarr[1].title.set_text('High Components')\naxarr[2].imshow(imgs[2])\naxarr[2].title.set_text('Enhanced')\n\n# Edge_filter\nimgs = edge_filter(img)\nplt.figure()\nf, axarr = plt.subplots(1,3) \nf.suptitle(\"Edge filter\", fontsize=14)\naxarr[0].imshow(imgs[0])\naxarr[0].title.set_text('Horizontal')\naxarr[1].imshow(imgs[1])\naxarr[1].title.set_text('Vertical')\naxarr[2].imshow(imgs[2])\naxarr[2].title.set_text('Filtered')","metadata":{"id":"dC9kwR380B-m","execution":{"iopub.status.busy":"2021-12-09T02:04:43.154049Z","iopub.execute_input":"2021-12-09T02:04:43.154946Z","iopub.status.idle":"2021-12-09T02:04:44.619347Z","shell.execute_reply.started":"2021-12-09T02:04:43.154907Z","shell.execute_reply":"2021-12-09T02:04:44.618521Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Data Pre-processing ","metadata":{"id":"65c59f4a"}},{"cell_type":"code","source":"# Usefull to convert id to name and name to id for y\nid_ref = {'big_cats': 0, 'butterfly':1, 'cat':2, 'chicken': 3, 'cow':4, 'dog':5, 'elephant': 6, 'goat': 7, 'horse': 8, 'spider': 9, 'squirrel':10}\nname_ref = {0:'big_cats', 1: 'butterfly', 2:'cat', 3:'chicken', 4:'cow', 5:'dog', 6:'elephant', 7:'goat', 8:'horse', 9:'spider', 10:'squirrel'}\n\n# \ndef find_original_y(Y):\n    \"\"\"\n    Convert one hot back to orinal\n    \"\"\"\n    y = []\n    for one_hot in Y:\n        for i in range(len(one_hot)):\n            if one_hot[i] == 1:\n                y.append(i)\n    if len(Y) != len(y):\n        raise Exception(\"Len not equal\")\n    return np.array(y)","metadata":{"id":"OMYVeGtY0He7","execution":{"iopub.status.busy":"2021-12-09T02:04:48.205499Z","iopub.execute_input":"2021-12-09T02:04:48.205758Z","iopub.status.idle":"2021-12-09T02:04:48.214149Z","shell.execute_reply.started":"2021-12-09T02:04:48.205731Z","shell.execute_reply":"2021-12-09T02:04:48.213219Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Usefull function for image processing\ndef process_image_flatt(img: np.ndarray):\n    \"\"\"\n    Normalize and flatt the image\n    \"\"\"\n    return (img / 255).flatten()\n\n\ndef process_image(img: np.ndarray):\n    \"\"\"\n    Normalize the image\n    \"\"\"\n    return (img / 255)\n\ndef resaphe(x):\n    nsamples, nx, ny = x.shape\n    return x.reshape((nsamples,nx*ny))\n\ndef process_images(imgs, process_func=process_image_flatt):\n    imgs_processed = []\n    for img in imgs:\n        imgs_processed.append(process_func(img))\n    return np.array(imgs_processed)","metadata":{"id":"EUy_pmNK0Pmy","execution":{"iopub.status.busy":"2021-12-09T02:04:50.060383Z","iopub.execute_input":"2021-12-09T02:04:50.060708Z","iopub.status.idle":"2021-12-09T02:04:50.068085Z","shell.execute_reply.started":"2021-12-09T02:04:50.060676Z","shell.execute_reply":"2021-12-09T02:04:50.067124Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Process imgs\n\n#### MODIFICATION HERE TO ADD FILTER ####\n# process_X =  np.array([edge_filter(img)[2] for img in process_images(X_raw, process_image)])\n# process_X =  np.array([gaussian_filter(img)[2] for img in process_images(X_raw, process_image)])\nprocess_X =  np.array([img for img in process_images(X_raw, process_image)])\nprocess_X.shape\n\n# Process y\nprocess_y = np.array([id_ref[name] for name in y_raw])\n# Encode the y vector to be in one hot format\nencoder = LabelEncoder()\nencoder.fit(process_y)\nencoded_Y = encoder.transform(process_y)\nprocess_Y_onehot = to_categorical(encoded_Y,num_classes=num_class)\n\n# Create a train and valid set (X and Y MAJ because dim > 1)\n# Partition: 0.25 x 0.8 = 0.2\nX_train, X_valid, Y_train, Y_valid = train_test_split(process_X, \n                                                      process_Y_onehot, \n                                                      test_size=0.20,  \n                                                      random_state=1)\n\n# Keep a little bit of data to evaluate the finale model for the repport\nX_valid, X_valid_2, Y_valid, Y_valid_2 = train_test_split(X_valid, Y_valid, test_size=0.12, random_state=1)\n\ny_valid = find_original_y(Y_valid)\ny_train = find_original_y(Y_train)\ny_valid_2 = find_original_y(Y_valid_2)","metadata":{"id":"o9lClr6V0Vk7","execution":{"iopub.status.busy":"2021-12-09T02:04:52.921077Z","iopub.execute_input":"2021-12-09T02:04:52.921461Z","iopub.status.idle":"2021-12-09T02:04:55.279601Z","shell.execute_reply.started":"2021-12-09T02:04:52.921375Z","shell.execute_reply":"2021-12-09T02:04:55.278574Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"unique, counts = np.unique(y_valid, return_counts=True)\n\nresult = np.column_stack((unique, counts)) \nnum_class = len(result)\nprint(f\"VALIDATION SET: {len(y_train)} datapoint\")\nprint(f\"The dataset have {num_class} class\")\nprint(\"Number of occ of each class in your data set\")\nfor r in result:\n  print(f\"{r[0]}: {r[1]} pictures\")","metadata":{"id":"d9e97957","execution":{"iopub.status.busy":"2021-12-09T02:05:00.046358Z","iopub.execute_input":"2021-12-09T02:05:00.047080Z","iopub.status.idle":"2021-12-09T02:05:00.054874Z","shell.execute_reply.started":"2021-12-09T02:05:00.047036Z","shell.execute_reply":"2021-12-09T02:05:00.053543Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"unique, counts = np.unique(y_train, return_counts=True)\n\nresult = np.column_stack((unique, counts)) \nnum_class = len(result)\nprint(f\"TRAINING SET: {len(y_valid)} datapoint\")\nprint(f\"The dataset have {num_class} class\")\nprint(\"Number of occ of each class in your data set\")\nfor r in result:\n  print(f\"{r[0]}: {r[1]} pictures\")","metadata":{"id":"Hw0fXjFixRv2","execution":{"iopub.status.busy":"2021-12-09T02:05:01.369953Z","iopub.execute_input":"2021-12-09T02:05:01.370274Z","iopub.status.idle":"2021-12-09T02:05:01.378733Z","shell.execute_reply.started":"2021-12-09T02:05:01.370239Z","shell.execute_reply":"2021-12-09T02:05:01.377963Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"unique, counts = np.unique(y_valid_2, return_counts=True)\n\nresult = np.column_stack((unique, counts)) \nnum_class = len(result)\nprint(f\"REPPORT TRAINING SET: {len(y_valid_2)} datapoint\")\nprint(f\"The dataset have {num_class} class\")\nprint(\"Number of occ of each class in your data set\")\nfor r in result:\n  print(f\"{r[0]}: {r[1]} pictures\")","metadata":{"id":"xZIN_GyCrLR2","execution":{"iopub.status.busy":"2021-12-09T02:05:03.039400Z","iopub.execute_input":"2021-12-09T02:05:03.039985Z","iopub.status.idle":"2021-12-09T02:05:03.047565Z","shell.execute_reply.started":"2021-12-09T02:05:03.039945Z","shell.execute_reply":"2021-12-09T02:05:03.046992Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Testing function","metadata":{"id":"8b37135b"}},{"cell_type":"code","source":"def f1_bench(clf, X, y, print_=True):\n    pred_values = clf.predict(X)\n    f1 = f1_score(y, pred_values, average='micro')\n    if print_:\n        print(f\"f1 score: {f1}\")\n    else:\n        return f1\n    \ndef f1_bench_nn(clf, X, y, print_=True):\n    preds = clf.predict(X)\n    pred_values = []\n    for p in preds:\n        pred_values.append(np.argmax(p))\n    f1 = f1_score(y, pred_values, average='micro')\n    if print_:\n        print(f\"f1 score: {f1}\")\n    else:\n        return f1","metadata":{"id":"8ef9c02a","execution":{"iopub.status.busy":"2021-12-09T02:05:05.360277Z","iopub.execute_input":"2021-12-09T02:05:05.360889Z","iopub.status.idle":"2021-12-09T02:05:05.367814Z","shell.execute_reply.started":"2021-12-09T02:05:05.360848Z","shell.execute_reply":"2021-12-09T02:05:05.367008Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Model SVM","metadata":{"id":"f81f16ef"}},{"cell_type":"code","source":"def data_split(X_t, y_t, X_v, y_v):\n    # train set with valid and train\n    X = np.concatenate((X_t, X_v), axis=0)\n    y = np.concatenate((y_t, y_v), axis=0)\n    \n    # Labal train and valid data\n    split_index = np.concatenate((np.full(X_t.shape[0], 1), np.zeros(X_v.shape[0])), axis=0)\n\n    return X, y, PredefinedSplit(test_fold=split_index)","metadata":{"id":"7RVvEzVMxkUd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test on a sub-set ","metadata":{"id":"gjjRsxQkxoKB"}},{"cell_type":"markdown","source":"### spider and cat","metadata":{"id":"xV0vZgop0tZ4"}},{"cell_type":"code","source":"def get_sub_set(X, y, animals):\n    x_sub = []\n    y_sub = []\n\n    for i in range(len(X)):\n        if y[i] in animals:\n            x_sub.append(X[i])\n            y_sub.append(y[i])\n\n    return np.array(x_sub), np.array(y_sub)\n\ndef print_parity(y):\n    unique, counts = np.unique(y, return_counts=True)\n    result = np.column_stack((unique, counts)) \n    print (result)\n\n# Araigner et chat seulement\nanimals = [9,2]\n\n# Traing sub data\nx_sub, y_sub = get_sub_set(X_train, y_train, animals)\n\n# Test sub data\nx_sub_t, y_sub_t = get_sub_set(X_valid, y_valid, animals)\n\nprint(f\"Number of train data: {len(x_sub)}\")\nprint(\"training: \")    \nprint_parity(y_sub)\n\nprint(f\"\\nNumber of test data: {len(x_sub_t)}\")\nprint(\"testing: \")\nprint_parity(y_sub_t)\n\nX_sub = resaphe(x_sub)\nX_sub_t = resaphe(x_sub_t)","metadata":{"id":"idEyYLy9xunZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n# Model with params optimisation\nclf = make_pipeline(StandardScaler(), LinearSVC(C=0.5, tol=1e-3))\nclf.fit(X_sub, y_sub)\n\n# Make Prediction\ny_pred = clf.predict(X_sub_t)\n\n# Print the accuracy\nprint(f\"The model is {accuracy_score(y_pred,y_sub_t)*100}% accurate\")","metadata":{"id":"bnCHDTYDxusU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model with params optimisation\nclf = make_pipeline(StandardScaler(), SVC(gamma='auto',  C=100))\nclf.fit(X_sub, y_sub)\n\n# Make Prediction\ny_pred = clf.predict(X_sub_t)\n\n# Print the accuracy\nprint(f\"The model is {accuracy_score(y_pred,y_sub_t)*100}% accurate\")","metadata":{"id":"o69-yeF7Oadp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Spider, cat, horse and goat","metadata":{"id":"zQp2dznL0pr1"}},{"cell_type":"code","source":"# Spider, cat, horse and goat\nanimals = [9, 2, 7, 8]\n\n# Traing sub data\nx_sub, y_sub = get_sub_set(X_train, y_train, animals)\n\n# Test sub data\nx_sub_t, y_sub_t = get_sub_set(X_valid, y_valid, animals)\n\nprint(f\"Number of train data: {len(x_sub)}\")\nprint(\"training: \")    \nprint_parity(y_sub)\n\nprint(f\"\\nNumber of test data: {len(x_sub_t)}\")\nprint(\"testing: \")\nprint_parity(y_sub_t)\nX_sub = resaphe(x_sub)\nX_sub_t = resaphe(x_sub_t)\n# Model with params optimisation\nclf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\nclf.fit(X_sub, y_sub)\n\n# Make Prediction\ny_pred = clf.predict(X_sub_t)\n\n# Print the accuracy\nprint(f\"The model is {accuracy_score(y_pred,y_sub_t)*100}% accurate\")","metadata":{"id":"Ne919r5MyhhX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## With all the anaimal","metadata":{"id":"u74IKamC00eR"}},{"cell_type":"code","source":"# Spider, cat, horse and goat\nanimals = [0,1,2,3,4,5,6,7,8,9,10]\n\n# Traing sub data\nx_sub, y_sub = get_sub_set(X_train, y_train, animals)\n\n# Test sub data\nx_sub_t, y_sub_t = get_sub_set(X_valid, y_valid, animals)\n\nprint(f\"Number of train data: {len(x_sub)}\")\nprint(\"training: \")    \nprint_parity(y_sub)\n\nprint(f\"\\nNumber of test data: {len(x_sub_t)}\")\nprint(\"testing: \")\nprint_parity(y_sub_t)","metadata":{"id":"5e-BHoI303C4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_sub = resaphe(x_sub)\nX_sub_t = resaphe(x_sub_t)\n# Model with params optimisation\nclf = make_pipeline(StandardScaler(), SVC(**{'kernel': 'poly'}))\nclf.fit(X_sub, y_sub)\n\n# Make Prediction\ny_pred = clf.predict(X_sub_t)\n\n# Print the accuracy\nprint(f\"The model is {accuracy_score(y_pred,y_sub_t)*100}% accurate\")","metadata":{"id":"t-hpJGho06pj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Grid search\nX_sub = resaphe(x_sub)\nX_sub_t = resaphe(x_sub_t)\nX, y, split = data_split(X_sub, y_sub, X_sub_t, y_sub_t)\n# test_params = {'C':[0.1,1,10,100],'gamma':[0.0001,0.001,0.1,1],'kernel':['rbf','poly']}\ntest_params = {'kernel':['rbf','poly', 'linear', 'sigmoid']}\n\nsvc=svm.SVC(probability=True)\n# clf=GridSearchCV(svc, param_grid)\n\ngrid_search = GridSearchCV(svc, param_grid=test_params, cv=split, refit=False, verbose=3, n_jobs=-1)\ngrid_search.fit(X, y)\nprint(grid_search.best_params_)\n\nSVM_model = LinearSVC(**grid_search.best_params_).fit(X, y)","metadata":{"id":"aGYrs26o073P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random forest","metadata":{"id":"ba779382"}},{"cell_type":"code","source":"def get_sub_set(X, y, animals):\n    x_sub = []\n    y_sub = []\n\n    for i in range(len(X)):\n        if y[i] in animals:\n            x_sub.append(X[i])\n            y_sub.append(y[i])\n\n    return np.array(x_sub), np.array(y_sub)\n\ndef print_parity(y):\n    unique, counts = np.unique(y, return_counts=True)\n    result = np.column_stack((unique, counts)) \n    print (result)\n\nanimals = [0,1,2,3,4,5,6,7,8,9,10,11]\n\n# Traing sub data\nx_sub, y_sub = get_sub_set(X_train, y_train, animals)\n\n# Test sub data\nx_sub_t, y_sub_t = get_sub_set(X_valid, y_valid, animals)\n\nprint(f\"Number of train data: {len(x_sub)}\")\nprint(\"training: \")    \nprint_parity(y_sub)\n\nprint(f\"\\nNumber of test data: {len(x_sub_t)}\")\nprint(\"testing: \")\nprint_parity(y_sub_t)\n\nX_sub = resaphe(x_sub)\nX_sub_t = resaphe(x_sub_t)","metadata":{"id":"QbIoj32o2Uhp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Depths = [50,45,40,35,30,25,None]\nPruning_Alpha = [10,100,1000]\nfor j in range(len(Pruning_Alpha)):\n    Model = RandomForestClassifier(criterion='gini',random_state=0,ccp_alpha=Pruning_Alpha[j])\n    Model.fit(X_sub, y_sub)\n    f1_bench(Model, X_sub_t, y_sub_t, print_=True)\n    Model = DecisionTreeClassifier(criterion='entropy',random_state=0,ccp_alpha=Pruning_Alpha[j])\n    Model.fit(X_sub, y_sub)\n    f1_bench(Model,X_sub_t, y_sub_t, print_=True)\n    print('alpha value =', Pruning_Alpha[j])\nfor j in range(len(Depths)):\n    Model = RandomForestClassifier(criterion='gini',random_state=0,max_depth = Depths[j])\n    Model.fit(X_sub, y_sub)\n    f1_bench(Model, X_sub_t, y_sub_t, print_=True)\n    Model = DecisionTreeClassifier(criterion='entropy',random_state=0,max_depth=Depths[j])\n    Model.fit(X_sub, y_sub)\n    f1_bench(Model, X_sub_t, y_sub_t, print_=True)\n    print('Depths Value =', Depths[j])\nn_estimator = [10,20,50,100,200, 500, 1000, 2000]\nfor j in range(len(n_estimator)):\n    Model = RandomForestClassifier(n_estimators = n_estimator[j], criterion='gini',random_state=0)\n    Model.fit(X_sub, y_sub)\n    f1_bench(Model, X_sub_t, y_sub_t, print_=True)\n    Model = RandomForestClassifier(n_estimators = n_estimator[j], criterion='entropy',random_state=0)\n    Model.fit(X_sub, y_sub)\n    f1_bench(Model, X_sub_t, y_sub_t, print_=True)\n    print('n value =', n_estimator[j])\n\nDepths = [50,45,40,35,30,25,None]\nVector_alpha_gini = [0.16610597140454164, 0.16610597140454164, 0.16610597140454164]\nVector_alpha_entropy = [0.16610597140454164, 0.16610597140454164, 0.16610597140454164]\nVector_Depths_gini = [0.33936080740117747, 0.33936080740117747, 0.34314550042052144, 0.340201850294365, 0.3435660218671152, 0.32253994953742643, 0.33936080740117747]\nVector_Depths_entropy = [0.19091673675357446, 0.19091673675357446, 0.19091673675357446, 0.19091673675357446, 0.19091673675357446, 0.19091673675357446, 0.19091673675357446]\nVector_n_gini = [0.255677039529016, 0.2994112699747687, 0.3301093355761144, 0.33936080740117747, 0.35407905803195955, 0.35912531539108494, 0.36206896551724144, 0.3549201009251472]\nVector_n_entropy = [0.255677039529016, 0.27544154751892347, 0.3036164844407065, 0.3288477712363331, 0.34146341463414637, 0.34440706476030275, 0.3528174936921783, 0.34524810765349034]\n\nplt.plot(Depths, Vector_Depths_gini, label = \"Gini\")\nplt.plot(Depths, Vector_Depths_entropy, label = \"Entropy\")\nplt.xlabel('Depths')\nplt.ylabel('f1 score')\nplt.legend()\nplt.show()\nplt.plot(n_estimator, Vector_n_gini, label = \"Gini\")\nplt.plot(n_estimator, Vector_n_entropy, label = \"Entropy\")\nplt.xlabel('n_estimators')\nplt.ylabel('f1 score')\nplt.legend()\nplt.show()","metadata":{"id":"ef67043a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN","metadata":{"id":"Qcm7EayX1CFy"}},{"cell_type":"markdown","source":"Creation of the Achitecuture model with a Convolutional Neural Network approach","metadata":{"id":"kJXHj2762oFb"}},{"cell_type":"code","source":"# Model Architecture \ndef create_model(init_conv_size, conv_size, dense_size):\n    model = models.Sequential()\n    model.add(layers.Conv2D(init_conv_size, (3, 3), strides=2, input_shape=(96, 96, 1)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation('relu'))\n\n    model.add(layers.MaxPooling2D((2, 2), strides=2))\n    model.add(layers.Conv2D(conv_size, (3, 3), strides=1, padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation('relu'))\n    model.add(layers.Conv2D(conv_size, (3, 3), strides=1, padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation('relu'))\n\n    model.add(layers.MaxPooling2D((2, 2), strides=2))\n    model.add(layers.Conv2D(conv_size*2, (3, 3), strides=1, padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation('relu'))\n    model.add(layers.Conv2D(conv_size*2, (3, 3), strides=1, padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation('relu'))\n\n    model.add(layers.MaxPooling2D((2, 2), strides=2))\n    model.add(layers.Conv2D(conv_size*2, (3, 3), strides=1, padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation('relu'))\n    model.add(layers.Conv2D(conv_size*2, (3, 3), strides=1, padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation('relu'))\n\n    model.add(layers.MaxPooling2D((2, 2), strides=2))\n    model.add(layers.Conv2D(conv_size*2, (3, 3), strides=1, padding='same'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation('relu'))\n    model.add(layers.MaxPooling2D((2, 2), strides=2))\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dense(dense_size, activation='relu'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(dense_size/2, activation='relu'))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dropout(0.65))\n    model.add(layers.Dense(11, activation='softmax'))\n    return model","metadata":{"id":"ksynygsq4F57","execution":{"iopub.status.busy":"2021-12-09T02:05:14.423444Z","iopub.execute_input":"2021-12-09T02:05:14.424268Z","iopub.status.idle":"2021-12-09T02:05:14.443636Z","shell.execute_reply.started":"2021-12-09T02:05:14.424196Z","shell.execute_reply":"2021-12-09T02:05:14.442752Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Model training","metadata":{"id":"dwYMr0A54gq_"}},{"cell_type":"code","source":"def lr_decay(epoch, lr):\n  \"\"\"\n  Return a reduced learning rate\n  \"\"\"\n  k = 0.02\n  return init_lr * math.exp(-k*epoch)\n","metadata":{"id":"WFHA-Qnt3qcR","execution":{"iopub.status.busy":"2021-12-09T02:05:16.203437Z","iopub.execute_input":"2021-12-09T02:05:16.203762Z","iopub.status.idle":"2021-12-09T02:05:16.208171Z","shell.execute_reply.started":"2021-12-09T02:05:16.203725Z","shell.execute_reply":"2021-12-09T02:05:16.207577Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"To find the same result as our Kaggle, pls do the following precedure of this section with the model 1, 2 and 3.\n\n1. Copy and paste each model in the following case\n2. Run the code for each section. Stop the model training at the specified epoch.\n\n\nModel 1:\n\n```\n# Trained with:\ntrain_datagen = ImageDataGenerator(\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\ntrain_generator = train_datagen.flow(X_train.reshape((len(X_train), 96, 96, 1)), Y_train, batch_size=16)\n\n# Random Seed\nseed = 100\nfix_seed_random(seed)\n\n# Model Name\nmodel_name = 'model_1'\n\n# Stoped at\n# Epoch=00069\n```\n\nModel 2:\n\n```\n# Trained with:\ntrain_datagen = ImageDataGenerator(\n        shear_range=0.18,\n        zoom_range=0.18,\n        horizontal_flip=True)\ntrain_generator = train_datagen.flow(X_train.reshape((len(X_train), 96, 96, 1)), Y_train, batch_size=16)\n\n# Random Seed\nseed = 200\nfix_seed_random(seed)\n\n# Model Name\nmodel_name = 'model_2'\n\n# Stoped at\n# Epoch=96\n```\n\nModel 3: With Gaussian filtered data !\n\n```\n# Trained with:\ntrain_datagen = ImageDataGenerator(\n        shear_range=0.22,\n        zoom_range=0.22,\n        horizontal_flip=True)\ntrain_generator = train_datagen.flow(X_train.reshape((len(X_train), 96, 96, 1)), Y_train, batch_size=16)\n\n# Random Seed\nseed = 300\nfix_seed_random(seed)\n\n# Model Name\nmodel_name = 'model_3'\n\n# Stoped at\n# Epoch=60\n```\n\nModel 4: With Edge filtered data !\n\n```\n# Trained with:\ntrain_datagen = ImageDataGenerator(\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\ntrain_generator = train_datagen.flow(X_train.reshape((len(X_train), 96, 96, 1)), Y_train, batch_size=16)\n\n# Random Seed\nseed = 400\nfix_seed_random(seed)\n\n# Model Name\nmodel_name = 'model_4'\n\n# Stoped at\n# Epoch=85\n```\n","metadata":{"id":"10wUfCRiphAH"}},{"cell_type":"code","source":"# Trained with:\ntrain_datagen = ImageDataGenerator(\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\ntrain_generator = train_datagen.flow(X_train.reshape((len(X_train), 96, 96, 1)), Y_train, batch_size=16)\n\n# Random Seed\nseed = 100\nfix_seed_random(seed)\n\n# Model Name\nmodel_name = 'model_1'\n\n","metadata":{"id":"cwdVNNeQswSF","execution":{"iopub.status.busy":"2021-12-09T02:05:38.222342Z","iopub.execute_input":"2021-12-09T02:05:38.222662Z","iopub.status.idle":"2021-12-09T02:05:38.387223Z","shell.execute_reply.started":"2021-12-09T02:05:38.222627Z","shell.execute_reply":"2021-12-09T02:05:38.386378Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Model creation\ninit_conv_sizes = 64\nconv_sizes = 128\ndense_sizes = 1024\nf1_scores = []\n\nmodel = create_model(init_conv_sizes, conv_sizes, dense_sizes)\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.categorical_crossentropy,\n              metrics=['accuracy'])\n\nmodel.summary()","metadata":{"id":"GAQ73kDv5tRi","execution":{"iopub.status.busy":"2021-12-09T02:05:40.653410Z","iopub.execute_input":"2021-12-09T02:05:40.653711Z","iopub.status.idle":"2021-12-09T02:05:41.290756Z","shell.execute_reply.started":"2021-12-09T02:05:40.653682Z","shell.execute_reply":"2021-12-09T02:05:41.290140Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Random Seed\nfix_seed_random(seed)\n\n# Learning rate\ninit_lr = 0.001\n\nmodel.fit(train_generator, steps_per_epoch = len(X_train) // 16, epochs=150, validation_data=(X_valid, Y_valid), callbacks=[LearningRateScheduler(lr_decay, verbose=1)])\nmicro_f1 = f1_bench_nn(model,X_valid_cnn,y_valid, False)\nf1_scores.append(micro_f1)","metadata":{"id":"R-OqB9yY4yGs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_bench_nn(model,X_train, y_train, True)\nf1_bench_nn(model,X_valid,y_valid, True)\nf1_bench_nn(model,X_valid_2,y_valid_2, True)\n\n# MAKE PREDICTION WITH THE DATA","metadata":{"id":"eeuOd7k5OctJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save The model","metadata":{"id":"gM80-dO16PFr"}},{"cell_type":"code","source":"# Save model\nfrom datetime import datetime\nmodel_path_pre_valid = f'saved_{model_name}/model_without_validation'\nmodel.save(model_path_pre_valid)","metadata":{"id":"QQhgkK_rtfhC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import a old model","metadata":{"id":"3O4Pufgx6Q3d"}},{"cell_type":"code","source":"# PATH TO A MODEL\npath_model_to_finish = f'saved_{model_name}/model_without_validation'\nmodel_ = tf.keras.models.load_model(path_model_to_finish)\n\n# Check the accuracy pre validation\nf1_bench_nn(model_ , X_train, y_train, True)\nf1_bench_nn(model_ , X_valid, y_valid, True)\nf1_bench_nn(model_,X_valid_2,y_valid_2, True)","metadata":{"id":"LdEOk908vrGb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add the validation Data ","metadata":{"id":"2KM9bR2J6Kd2"}},{"cell_type":"code","source":"# ~~~~~~~~~~ TRAIN WITH GENERATOR ~~~~~~~~~~\nfix_seed_random(seed)\nvalid_generator = train_datagen.flow(X_valid.reshape((len(X_valid), 96, 96, 1)), Y_valid, batch_size=16)\ninit_lr = 0.0003\nepochs_valid = 12\nmodel_.fit(valid_generator, steps_per_epoch = len(X_valid) // 16, epochs=epochs_valid, validation_data=(X_train, Y_train), callbacks=[LearningRateScheduler(lr_decay, verbose=1)])\n\nf1_bench_nn(model_,X_train, y_train, True)\nf1_bench_nn(model_,X_valid,y_valid, True)\nf1_bench_nn(model_,X_valid_2,y_valid_2, True)\nmodel_.save(f'saved_{model_name}/model_generator_validation')","metadata":{"id":"xwopVo-1roUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ~~~~~~~~~~ TRAIN WITH VALIDATION ~~~~~~~~~~\nfix_seed_random(seed)\npath_model_to_finish = f'saved_{model_name}/model_without_validation'\nmodel_ = tf.keras.models.load_model(path_model_to_finish)\n\nmodel_.fit(X_valid, Y_valid, epochs=8, batch_size=16)\n\nf1_bench_nn(model_,X_train, y_train, True)\nf1_bench_nn(model_,X_valid,y_valid, True)\nf1_bench_nn(model_,X_valid_2,y_valid_2, True)\n\nmodel_.save(f'saved_{model_name}/model_validation')","metadata":{"id":"UwxuUUsr50I9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make prediction","metadata":{"id":"DqC4IU1VCKE2"}},{"cell_type":"code","source":"def create_pred_file(preds, file_name='df_remise.csv'):\n    ids = [i for i in range(len(preds))]\n    df_remise = pd.DataFrame({'Id': ids,\n                          'class': preds})\n    df_remise.to_csv(file_name, sep=',', index=False)\n\n# USE THE SAME PREPROCESSING AS THE DATA USE FOR TRAINING !!!!!!!\n# X_test =  np.array([edge_filter(img)[2] for img in process_images(x_test_raw, process_image)])\n# X_test =  np.array([gaussian_filter(img)[2] for img in process_images(x_test_raw, process_image)])\n# X_test = np.array([img for img in process_images(x_test_raw, process_image)])","metadata":{"id":"ETt8Dk23CUcU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# With Normal data\nfiles = [\n      'saved_model_1/model_validation',\n      'saved_model_2/model_validation'\n]\nX_test = np.array([img for img in process_images(x_test_raw, process_image)])\nX_test_cnn = np.array([x.reshape((96,96,1)) for x in X_test])\n\nfor model_file in files:\n  model_ = tf.keras.models.load_model(model_file)\n  preds = model_.predict(X_test_cnn)\n  y_pred = []\n  for p in preds:\n      y_pred.append(np.argmax(p))\n  pred_file_name = f'{model_file}.csv'\n  create_pred_file(y_pred, pred_file_name)","metadata":{"id":"n_WVXFo78qw8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# With Gaussian      \nfiles = [\n      'saved_model_3/model_validation',\n      'saved_model_3/model_without_validation',\n      'saved_model_3/model_generator_validation'\n]\n\nX_test =  np.array([gaussian_filter(img)[2] for img in process_images(x_test_raw, process_image)])\nX_test_cnn = np.array([x.reshape((96,96,1)) for x in X_test])\n\nfor model_file in files:\n  model_ = tf.keras.models.load_model(model_file)\n  preds = model_.predict(X_test_cnn)\n  y_pred = []\n  for p in preds:\n      y_pred.append(np.argmax(p))\n  pred_file_name = f'{model_file}.csv'\n  create_pred_file(y_pred, pred_file_name)","metadata":{"id":"XHPX8P1uy3dU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Essemble","metadata":{"id":"A72dVivcGLhQ"}},{"cell_type":"markdown","source":"Testing Essemble model","metadata":{"id":"UnOWLckMYzAr"}},{"cell_type":"code","source":"# Test on valid set 2: Exemple with Model_3\nmodel_files = [\n      'saved_model_3/model_without_validation',\n      'saved_model_3/model_validation',\n      'saved_model_3/model_generator_validation',\n]\n\npreds = [[] for _ in range(len(y_valid_2))]\nfor model_file in model_files:\n  model_ = tf.keras.models.load_model(model_file)\n  pred = model_.predict(X_valid_2)\n  for i in range(len(pred)):\n    preds[i].append(np.argmax(pred[i]))\n\nprint(preds)\n\nimport random\ndef most_frequent(List):\n    counter = 1\n    num = random.choice(List)\n    \n    for i in List:\n        curr_frequency = List.count(i)\n        if(curr_frequency> counter):\n            counter = curr_frequency\n            num = i\n \n    return num\n\n    y_pred = []\nfor i in range(len(y_valid_2)):\n    pred_arr = preds[i]\n    y_pred.append(most_frequent(pred_arr))\n\ngood_pred = 0\nfor i in range(len(y_valid_2)):\n  if y_valid_2[i] == y_pred[i]:\n    good_pred += 1\n\nprint(good_pred/len(y_valid_2))","metadata":{"id":"zd0yw1HxUlZS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using Model Essemble on test set","metadata":{"id":"KDI0Wmt5Y3tR"}},{"cell_type":"code","source":"model_pred_df = [\n    # pd.read_csv(\"Pred/model_1_validation.csv\"),\n    # pd.read_csv(\"Pred/model_2_validation.csv\"),\n    # pd.read_csv(\"Pred/model_3_validation.csv\"),\n    # pd.read_csv(\"Pred/model_3_without_validation.csv\"),\n\n    pd.read_csv('saved_model_1/model_validation.csv'), \n    pd.read_csv('saved_model_2/model_validation.csv'),\n    pd.read_csv('saved_model_3/model_validation.csv'),\n    pd.read_csv('saved_model_3/model_without_validation.csv')\n]\n\ndef get_pred(i):\n  pred = []\n  for j in range(len(model_pred_df)):\n    pred.append(model_pred_df[j].iloc[i]['class'])\n  return pred","metadata":{"id":"ygIBa2GQo3gz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploration\ni = 1\npred_i = get_pred(i)\nprint(pred_i)\n\n# Check if behavior is ok\nprint(most_frequent(pred_i))","metadata":{"id":"-uSrtmXVH0w_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate file with combinaison\npreds = []\nfor i in range(len(model_pred_df[0])):\n    pred_arr = get_pred(i)\n    preds.append(most_frequent(pred_arr))\n\ncreate_pred_file(preds, \"df_remise_multiple_model.csv\")","metadata":{"id":"iU89dU6qo4M-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r /content/file.zip /content\nfrom google.colab import files\nfiles.download(\"/content/file.zip\")","metadata":{"id":"c00tW8STMvm0"},"execution_count":null,"outputs":[]}]}