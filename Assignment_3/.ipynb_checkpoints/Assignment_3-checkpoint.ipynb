{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Guillaume Thibault - Matricule 1948612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "data_path = './medical_dataset/'\n",
    "submit_path = './hwk3_submit/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Text Classification\n",
    "\n",
    "In this assignment, we will build a classifier with medical-NLP corpus. This is a classification\n",
    "task with the input as a medical transcription (text) and the output as the corresponding\n",
    "medical transcript type. This is a clinical dataset which consists of a medical transcript from\n",
    "one of the 4 classes {Surgery - 1 , Medical Records - 2, Internal Medicine - 3 and Other - 4}\n",
    "as an input. The task is to classify the transcript (text) to the corresponding classes i.e the\n",
    "transcript type. The dataset consists of 4000 transcripts in the training set and 500 in each\n",
    "of the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "pre_train = pd.read_csv(data_path + 'train.csv', sep=\",\")\n",
    "pre_valid = pd.read_csv(data_path + 'valid.csv', sep=\",\")\n",
    "pre_test = pd.read_csv(data_path + 'test.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vectorisation of natural language text [10 marks]  \n",
    "\n",
    "Most of the algorithms described in the class take the input as a vector. However, the\n",
    "reviews are natural language text of varying number of words. The first step would be to\n",
    "convert this varying-length movie review to a fixed-length vector representation. We will\n",
    "consider two different ways of vectorizing the natural language text: binary bag-of-\n",
    "words representation and frequency bag-of-words representation\n",
    "\n",
    "Instructions for dataset submission are given in the end of the assignment (do not include the dataset in the report). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "def add_word_occ_to_dict(words: str, occ_dict: dict) -> None:\n",
    "    \"\"\"\n",
    "    Add word occurence to a dictionnary\n",
    "    @params:\n",
    "        - words (str): conatining the words separeted by ' '\n",
    "        - occ_dict (dict): dictionnay containg the words as the keys and the occurence number as a value\n",
    "    \"\"\"\n",
    "    word = words.split(' ')   \n",
    "    sorted_= sorted(set(word))\n",
    "    for val_sort in sorted_:\n",
    "        if val_sort in occ_dict.keys():\n",
    "            occ_dict[val_sort] += word.count(val_sort) \n",
    "        else:\n",
    "            occ_dict[val_sort] = word.count(val_sort) \n",
    "\n",
    "\n",
    "def remove_ponctuaion(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove all ponctuations from a string\n",
    "    @params: \n",
    "        - string (str): string to remove ponct.\n",
    "    \"\"\"\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "\n",
    "def sort_tuple(tup, most_freq=True): \n",
    "    \"\"\"\n",
    "    key is set to sort using second element of sublist lambda has been used \n",
    "    \"\"\"\n",
    "    tup.sort(key = lambda x: x[1], reverse=most_freq) \n",
    "    return tup\n",
    "\n",
    "\n",
    "def get_top_n_occ(df, n: int)-> list:\n",
    "    occ = {}\n",
    "    for index, row in df.iterrows():\n",
    "        text = remove_ponctuaion(row[1]).lower() # Lower case all char\n",
    "        add_word_occ_to_dict(text, occ)\n",
    "    sorted_tup = sort_tuple([(k, v) for k, v in occ.items()])\n",
    "    return [sorted_tup[i] for i in range(0, n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Binary Bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_bag_of_word(df, n) -> pd.DataFrame:\n",
    "    # Step 1 and 2\n",
    "    top_n_words_tup = get_top_n_occ(df, n)\n",
    "    cols = [k for k, v in top_n_words_tup]\n",
    "    \n",
    "    \n",
    "    # Step 3\n",
    "    bow_df = pd.DataFrame(columns=[\"label\"] + cols)\n",
    "    for index, row in df.iterrows():\n",
    "        bow_row = [row[0]]\n",
    "        text = remove_ponctuaion(row[1]).lower()\n",
    "        word_set = sorted(set(text.split(' ')))\n",
    "        for word in cols:\n",
    "            if word in word_set:\n",
    "                bow_row.append(1)\n",
    "            else:\n",
    "                bow_row.append(0)\n",
    "        bow_df.loc[len(bow_df)] = bow_row\n",
    "    return bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 919.4922947883606s seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Process dataset - Binary Bag of word\n",
    "n = 10000\n",
    "b_test = binary_bag_of_word(pre_test, n)  \n",
    "b_train = binary_bag_of_word(pre_train, n)  \n",
    "b_valid = binary_bag_of_word(pre_valid, n)  \n",
    "\n",
    "print(f\"--- {(time.time() - start_time)}s seconds ---\")\n",
    "\n",
    "# Save bag of words dataframe\n",
    "b_test.to_csv(submit_path + 'b_test.csv')\n",
    "b_valid.to_csv(submit_path + 'b_valid.csv')\n",
    "b_train.to_csv(submit_path + 'b_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bag of words dataframe from file\n",
    "b_test = pd.read_csv(submit_path + 'b_test.csv', sep=\",\")\n",
    "b_valid = pd.read_csv(submit_path + 'b_valid.csv', sep=\",\")\n",
    "b_train = pd.read_csv(submit_path + 'b_train.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Frequency Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_bag_of_word(df, n) -> pd.DataFrame:\n",
    "    # Step 1 and 2\n",
    "    top_n_words_tup = get_top_n_occ(df, n)\n",
    "    cols = [k for k, v in top_n_words_tup]\n",
    "    \n",
    "    # Step 3\n",
    "    bow_df = pd.DataFrame(columns=[\"label\"] + cols)\n",
    "    for index, row in df.iterrows():\n",
    "        bow_row = [row[0]]\n",
    "        text = remove_ponctuaion(row[1]).lower()\n",
    "        word_set = sorted(set(text.split(' ')))\n",
    "        for word_tup in top_n_words_tup:\n",
    "            bow_row.append(text.count(word_tup[0]) / word_tup[1])\n",
    "        bow_df.loc[len(bow_df)] = bow_row\n",
    "        \n",
    "    return bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 585.1088128089905s seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Process dataset - Frequency Bag of word\n",
    "n = 10000\n",
    "f_test = frequency_bag_of_word(pre_test, n)  \n",
    "f_valid = frequency_bag_of_word(pre_valid, n)  \n",
    "f_train = frequency_bag_of_word(pre_train, n) \n",
    "\n",
    "print(f\"--- {(time.time() - start_time)}s seconds ---\")\n",
    "\n",
    "# Save bag of words dataframe\n",
    "f_test.to_csv(submit_path + 'f_test.csv')\n",
    "f_valid.to_csv(submit_path + 'f_valid.csv')\n",
    "f_train.to_csv(submit_path + 'f_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bag of words dataframe from file\n",
    "f_test = pd.read_csv(submit_path + 'f_test.csv', sep=\",\")\n",
    "f_valid = pd.read_csv(submit_path + 'f_valid.csv', sep=\",\")\n",
    "f_train = pd.read_csv(submit_path + 'f_train.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_vocab(vocab):\n",
    "    with open('medical_nlp-vocab.txt', 'w') as f:\n",
    "        for i in range(1, len(vocab) + 1):\n",
    "            f.writelines(f\"{vocab[i-1][0]}\\t{i}\\t{vocab[i-1][1]}\\n\")\n",
    "            \n",
    "def submit_set(prefix, df, vocab):\n",
    "    cols = [k for k, v in vocab]\n",
    "    \n",
    "    with open(f'medical {prefix}-traint.txt', 'w') as f:\n",
    "        for index, row in df.iterrows():\n",
    "            for i in range(len(cols)):\n",
    "                if row[i+1] != 0:\n",
    "                    f.write(f'{i+1} ')\n",
    "            f.write(f'\\t{row[0]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "\n",
    "submit_vocab(get_top_n_occ(pre_train, n))\n",
    "submit_set('test', b_test, get_top_n_occ(pre_test, n)) \n",
    "submit_set('valid', b_valid, get_top_n_occ(pre_valid, n)) \n",
    "submit_set('train', b_train, get_top_n_occ(pre_train, n)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 2. Models using binary bag-of-words [19 marks]\n",
    "\n",
    "For this question, we will focus on the Medical-NLP dataset with binary bag-of-words\n",
    "(BBoW) representation. We will use the F1-score as the evaluation metric for the entire\n",
    "assignment. \n",
    "\n",
    "### (a) Random classifier [2 marks]\n",
    "As a baseline, report the performance of the random classifier (a classifier which\n",
    "classifies a review into a uniformly random class) and the majority-class classifier\n",
    "(a classifier which computes the majority class in the training set and classifies all\n",
    "test instances as that majority class). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Random classifier\n",
    "def random_model_f1(df):\n",
    "    n_value = len(df)\n",
    "    possible_value = df['label'].unique()\n",
    "    pred_values = [ possible_value[int(np.random.uniform(low=0.0, high=0.9999, size=None) * len(possible_value))] for _ in range(n_value)]\n",
    "    pred_values = np.array(pred_values)\n",
    "    real_values = df['label'].to_numpy()\n",
    "    f1 = f1_score(real_values, pred_values, labels=possible_value, average='macro')\n",
    "    return f1\n",
    "    \n",
    "def most_freq_model_f1(df):\n",
    "    real_values = df['label'].to_numpy()\n",
    "    counts = np.bincount(real_values)\n",
    "    most_freq_value = np.argmax(counts)\n",
    "    pred_values = [most_freq_value] * len(real_values)\n",
    "    f1 = f1_score(real_values, pred_values, labels=df['label'].unique(), average='macro')\n",
    "    return f1\n",
    "\n",
    "def test_model(model, fn, df):\n",
    "    print(f\"f1 {model}_set score: {fn(df)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ~~ Random Model ~~\n",
      "f1 train_set on random_model score: 0.2552066930419413\n",
      "f1 valid_set on random_model score: 0.2574430583248011\n",
      "f1 test_set on random_model score: 0.2635717389954678\n",
      "\n",
      "\n",
      " ~~ Most Frequence Model ~~\n",
      "f1 train_set on random_model score: 0.120996778472617\n",
      "f1 valid_set on random_model score: 0.12424698795180723\n",
      "f1 test_set on random_model score: 0.14183381088825217\n"
     ]
    }
   ],
   "source": [
    "print(\" ~~ Random Model ~~\")\n",
    "test_model('train', random_model_f1, b_train)\n",
    "test_model('valid', random_model_f1, b_valid)\n",
    "test_model('test', random_model_f1, b_test)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\" ~~ Most Frequence Model ~~\")\n",
    "test_model('train', most_freq_model_f1, b_train)\n",
    "test_model('valid', most_freq_model_f1, b_valid)\n",
    "test_model('test', most_freq_model_f1, b_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Naive Bayes, Decision Trees, Logistic regression and Linear SVM [8 marks]\n",
    "Now train Naive Bayes, Decision Trees, Logistic regression and Linear SVM for this\n",
    "task. \n",
    "\n",
    "[Note: You should do a thorough hyper-parameter tuning by using the given\n",
    "validation set. Also, note that you should use the appropriate naive Bayes classifier\n",
    "for binary input features (also called Bernoulli naive Bayes).] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(df):\n",
    "    X = df.drop(columns=['label']).to_numpy()\n",
    "    y = df['label'].to_numpy()\n",
    "    return X, y\n",
    "\n",
    "def f1_bench(clf, df):\n",
    "    # Prediction\n",
    "    X, real_values = extract_data(df)\n",
    "    pred_values = clf.predict(X)\n",
    "    \n",
    "    # f1 score\n",
    "    f1 = f1_score(real_values, pred_values, labels=df_test['label'].unique(), average='macro')\n",
    "    print(f\"f1 score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.19590225563909774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "def bernoulli_naive_bayes(df, **kwargs):\n",
    "    X, y = extract_data(df)\n",
    "    return BernoulliNB(**kwargs).fit(X, y)\n",
    "\n",
    "clf = bernoulli_naive_bayes(b_train)\n",
    "\n",
    "f1_bench(clf, b_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.3159825974867466\n"
     ]
    }
   ],
   "source": [
    "def decision_tree(df, **kwargs):\n",
    "    X, y = extract_data(df)\n",
    "    return DecisionTreeClassifier(**kwargs).fit(X, y)\n",
    "\n",
    "clf = decision_tree(b_train)\n",
    "\n",
    "f1_bench(clf, b_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.2677315221096108\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(df, **kwargs):\n",
    "    X, y = extract_data(df)\n",
    "    return LogisticRegression(**kwargs).fit(X, y)\n",
    "\n",
    "clf = logistic_regression(b_train)\n",
    "\n",
    "f1_bench(clf, b_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.22382720790368255\n"
     ]
    }
   ],
   "source": [
    "def SVM(df, **kwargs):\n",
    "    X, y = extract_data(df)\n",
    "    clf = make_pipeline(StandardScaler(), SVC(**kwargs))\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "clf = SVM(b_train)\n",
    "\n",
    "f1_bench(clf, b_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Report the list of hyper-parameters [3 marks]\n",
    "Report the list of hyper-parameters you considered for each classifier, their range,\n",
    "as well as the best values for these hyper-parameters, chosen based on the validation\n",
    "set performance1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.21121649184149183\n"
     ]
    }
   ],
   "source": [
    "# Naive bayes\n",
    "hyper_params = {\n",
    "    'alpha': 1.0e-10,\n",
    "    'binarize': 0.0,\n",
    "    'fit_prior': True\n",
    "}\n",
    "clf = bernoulli_naive_bayes(b_train, **hyper_params)\n",
    "\n",
    "f1_bench(clf, b_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.3613305487071276\n"
     ]
    }
   ],
   "source": [
    "hyper_params = {\n",
    "    'criterion':\"gini\",\n",
    "    'splitter':\"best\", \n",
    "    'max_depth': 4000,\n",
    "    'min_samples_split': 2, \n",
    "    'min_samples_leaf' : 10,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'max_features': None,\n",
    "    'random_state': 1000,\n",
    "    'max_leaf_nodes': 100,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'class_weight': None\n",
    "}\n",
    "clf = decision_tree(b_train, **hyper_params)\n",
    "\n",
    "f1_bench(clf, b_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.262772782271148\n"
     ]
    }
   ],
   "source": [
    "hyper_params = { \n",
    "    'solver':'newton-cg', \n",
    "    'penalty': 'l2', \n",
    "    'C':1.0, \n",
    "    'dual': False,\n",
    "    'tol': 1e-4,\n",
    "    'fit_intercept': True,\n",
    "    'random_state': None,\n",
    "    'intercept_scaling': 1,\n",
    "    'max_iter':100, \n",
    "    'multi_class':'multinomial'\n",
    "}\n",
    "clf = logistic_regression(b_train, **hyper_params)\n",
    "\n",
    "f1_bench(clf, b_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X.shape[0] should be equal to X.shape[1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-324-96ac1f236509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m'random_state'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m }\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhyper_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mf1_bench\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-215-0554bd79ea06>\u001b[0m in \u001b[0;36mSVM\u001b[0;34m(df, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"precomputed\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X.shape[0] should be equal to X.shape[1]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: X.shape[0] should be equal to X.shape[1]"
     ]
    }
   ],
   "source": [
    "hyper_params = {\n",
    "    'gamma':'auto',\n",
    "    'kernel': 'precomputed',\n",
    "    'degree': 3,\n",
    "    'coef0': 0.0,\n",
    "    'shrinking': True,\n",
    "    'probability': False,\n",
    "    'tol': 1e-3,\n",
    "    'cache_size': 200,\n",
    "    'max_iter': -1,\n",
    "    'random_state': None\n",
    "}\n",
    "clf = SVM(b_train, **hyper_params)\n",
    "\n",
    "f1_bench(clf, b_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) F1-score [3 marks]\n",
    "Report the training, validation, and test F1-score for all the classifiers  (with best hyper-parameter configuration). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear: 0.23550706333938282\n",
    "sigmoid: 0.2426287107763022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Comment on the performance of different classifiers. [3 marks]\n",
    "\n",
    "Why did a particular classifier\n",
    "perform better than the rest? What was the role of the hyper-parameters in finding\n",
    "the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model using frequency bag-of-words [21 marks]\n",
    "\n",
    "Now we will repeat question 2 but with frequency bag-of-words (FBoW) representation.\n",
    "\n",
    "### (a) Naive Bayes, Decision Tree, Logistic regression and Linear SVM  [8 marks]\n",
    "Train Naive Bayes, Decision Tree, Logistic regression and Linear SVM for this task.\n",
    "\n",
    "[Note: Again, you should do a thorough hyper-parameter tuning by using the given\n",
    "validation set. Also, note that you should use the appropriate naive Bayes classifier\n",
    "for real valued input features (also called Gaussian naive Bayes).] [8 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def gaussian_naive_bayes(df, **kwargs):\n",
    "    X, y = extract_data(df)\n",
    "    return BernoulliNB(**kwargs).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.10999999999999999\n",
      "f1 score: 0.22872563336704382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.1757513807786575\n",
      "f1 score: 0.090311986863711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = bernoulli_naive_bayes(f_train)\n",
    "f1_bench(clf, f_valid)\n",
    "\n",
    "clf = decision_tree(f_train)\n",
    "f1_bench(clf, f_valid)\n",
    "\n",
    "clf = logistic_regression(f_train)\n",
    "f1_bench(clf, f_valid)\n",
    "\n",
    "clf = SVM(f_train)\n",
    "f1_bench(clf, f_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Report the list of hyper-parameters [3 marks]\n",
    "Report the list of hyper-parameters you considered for each classifier, their range,\n",
    "as well as the best values for these hyper-parameters, chosen based on the validation\n",
    "set performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'min_samples_split':10, \n",
    "    'splitter':\"random\", \n",
    "    'criterion':\"gini\"\n",
    "}\n",
    "clf = decision_tree(b_train, **hyper_params)\n",
    "\n",
    "f1_bench(clf, b_valid)\n",
    "\n",
    "hyper_params = { \n",
    "    'penalty':'l2', \n",
    "    'C':0.01, \n",
    "    'solver':'newton-cg', \n",
    "    'max_iter':1000, \n",
    "    'multi_class':'multinomial'\n",
    "}\n",
    "clf = logistic_regression(b_train, **hyper_params)\n",
    "\n",
    "f1_bench(clf, b_valid)\n",
    "\n",
    "hyper_params = {\n",
    "    'gamma':'auto'\n",
    "}\n",
    "clf = SVM(b_train, **hyper_params)\n",
    "\n",
    "f1_bench(clf, b_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) F1-score [3 marks]\n",
    "Report the training, validation, and test F1-score for all the classifiers (with best\n",
    "hyper-parameter configuration). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Comment on the performance of different classifiers [3 mark]\n",
    "Why did a particular classifier perform better than the rest? What was the role of the hyper-parameters in finding\n",
    "the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Compare the performance with the binary bag-of-words based classifiers. [2 mark]\n",
    "\n",
    "Why is\n",
    "there a difference in the performance? Give a brief explanation comparing BBoW\n",
    "Naive Bayes and FBoW Naive Bayes and similarly for other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f)  Comment on the representation [2 mark]\n",
    "Which representation is better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(f\"--- {(time.time() - start_time)}s seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
