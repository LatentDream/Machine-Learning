{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Guillaume Thibault - Matricule 1948612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "data_path = './medical_dataset/'\n",
    "submit_path = './hwk3_submit/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Text Classification\n",
    "\n",
    "In this assignment, we will build a classifier with medical-NLP corpus. This is a classification\n",
    "task with the input as a medical transcription (text) and the output as the corresponding\n",
    "medical transcript type. This is a clinical dataset which consists of a medical transcript from\n",
    "one of the 4 classes {Surgery - 1 , Medical Records - 2, Internal Medicine - 3 and Other - 4}\n",
    "as an input. The task is to classify the transcript (text) to the corresponding classes i.e the\n",
    "transcript type. The dataset consists of 4000 transcripts in the training set and 500 in each\n",
    "of the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2-D STUDY,1. Mild aortic stenosis, widely calc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PREOPERATIVE DIAGNOSES: , Dysphagia and esopha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>CHIEF COMPLAINT:,  The patient comes for three...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>PROCEDURE: , Bilateral L5, S1, S2, and S3 radi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>DISCHARGE DIAGNOSES:,1. Chronic obstructive pu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      2  2-D STUDY,1. Mild aortic stenosis, widely calc...\n",
       "1      1  PREOPERATIVE DIAGNOSES: , Dysphagia and esopha...\n",
       "2      2  CHIEF COMPLAINT:,  The patient comes for three...\n",
       "3      1  PROCEDURE: , Bilateral L5, S1, S2, and S3 radi...\n",
       "4      2  DISCHARGE DIAGNOSES:,1. Chronic obstructive pu..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "pre_train = pd.read_csv(data_path + 'train.csv', sep=\",\")\n",
    "pre_valid = pd.read_csv(data_path + 'valid.csv', sep=\",\")\n",
    "pre_test = pd.read_csv(data_path + 'test.csv', sep=\",\")\n",
    "pre_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vectorisation of natural language text [10 marks]  \n",
    "\n",
    "Most of the algorithms described in the class take the input as a vector. However, the\n",
    "reviews are natural language text of varying number of words. The first step would be to\n",
    "convert this varying-length movie review to a fixed-length vector representation. We will\n",
    "consider two different ways of vectorizing the natural language text: binary bag-of-\n",
    "words representation and frequency bag-of-words representation\n",
    "\n",
    "Instructions for dataset submission are given in the end of the assignment (do not include the dataset in the report). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "def add_word_occ_to_dict(words: str, occ_dict: dict) -> None:\n",
    "    \"\"\"\n",
    "    Add word occurence to a dictionnary\n",
    "    @params:\n",
    "        - words (str): conatining the words separeted by ' '\n",
    "        - occ_dict (dict): dictionnay containg the words as the keys and the occurence number as a value\n",
    "    \"\"\"\n",
    "    word = words.split(' ')   \n",
    "    sorted_= sorted(set(word))\n",
    "    for val_sort in sorted_:\n",
    "        if val_sort in occ_dict.keys():\n",
    "            occ_dict[val_sort] += word.count(val_sort) \n",
    "        else:\n",
    "            occ_dict[val_sort] = word.count(val_sort) \n",
    "\n",
    "\n",
    "def remove_ponctuaion(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove all ponctuations from a string\n",
    "    @params: \n",
    "        - string (str): string to remove ponct.\n",
    "    \"\"\"\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "\n",
    "def sort_tuple(tup, most_freq=True): \n",
    "    \"\"\"\n",
    "    key is set to sort using second element of sublist lambda has been used \n",
    "    \"\"\"\n",
    "    tup.sort(key = lambda x: x[1], reverse=most_freq) \n",
    "    return tup\n",
    "\n",
    "\n",
    "def get_top_n_occ(df, n: int)-> list:\n",
    "    occ = {}\n",
    "    for index, row in df.iterrows():\n",
    "        text = remove_ponctuaion(row[1]).lower() # Lower case all char\n",
    "        add_word_occ_to_dict(text, occ)\n",
    "    sorted_tup = sort_tuple([(k, v) for k, v in occ.items()])\n",
    "    return [sorted_tup[i] for i in range(0, n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Binary Bag of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_bag_of_word(df, n) -> pd.DataFrame:\n",
    "    # Step 1 and 2\n",
    "    top_n_words_tup = get_top_n_occ(df, n)\n",
    "    cols = [k for k, v in top_n_words_tup]\n",
    "    \n",
    "    \n",
    "    # Step 3\n",
    "    bow_df = pd.DataFrame(columns=[\"label\"] + cols)\n",
    "    for index, row in df.iterrows():\n",
    "        bow_row = [row[0]]\n",
    "        text = remove_ponctuaion(row[1]).lower()\n",
    "        word_set = sorted(set(text.split(' ')))\n",
    "        for word in cols:\n",
    "            if word in word_set:\n",
    "                bow_row.append(1)\n",
    "            else:\n",
    "                bow_row.append(0)\n",
    "        bow_df.loc[len(bow_df)] = bow_row\n",
    "    return bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 919.4922947883606s seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Process dataset - Binary Bag of word\n",
    "n = 10000\n",
    "b_test = binary_bag_of_word(pre_test, n)  \n",
    "b_train = binary_bag_of_word(pre_train, n)  \n",
    "b_valid = binary_bag_of_word(pre_valid, n)  \n",
    "\n",
    "b_train = b_train.loc[:, ~b_train.columns.str.contains('^Unnamed')]\n",
    "b_test = b_test.loc[:, ~b_test.columns.str.contains('^Unnamed')]\n",
    "b_valid = b_valid.loc[:, ~b_valid.columns.str.contains('^Unnamed')]\n",
    "\n",
    "print(f\"--- {(time.time() - start_time)}s seconds ---\")\n",
    "\n",
    "# Save bag of words dataframe\n",
    "b_test.to_csv(submit_path + 'b_test.csv')\n",
    "b_valid.to_csv(submit_path + 'b_valid.csv')\n",
    "b_train.to_csv(submit_path + 'b_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bag of words dataframe from file\n",
    "b_test = pd.read_csv(submit_path + 'b_test.csv', sep=\",\")\n",
    "b_valid = pd.read_csv(submit_path + 'b_valid.csv', sep=\",\")\n",
    "b_train = pd.read_csv(submit_path + 'b_train.csv', sep=\",\")\n",
    "\n",
    "b_train = b_train.loc[:, ~b_train.columns.str.contains('^Unnamed')]\n",
    "b_test = b_test.loc[:, ~b_test.columns.str.contains('^Unnamed')]\n",
    "b_valid = b_valid.loc[:, ~b_valid.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Frequency Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_bag_of_word(df, n) -> pd.DataFrame:\n",
    "    # Step 1 and 2\n",
    "    top_n_words_tup = get_top_n_occ(df, n)\n",
    "    cols = [k for k, v in top_n_words_tup]\n",
    "    \n",
    "    # Step 3\n",
    "    bow_df = pd.DataFrame(columns=[\"label\"] + cols)\n",
    "    for index, row in df.iterrows():\n",
    "        bow_row = [row[0]]\n",
    "        text = remove_ponctuaion(row[1]).lower()\n",
    "        word_set = sorted(set(text.split(' ')))\n",
    "        for word_tup in top_n_words_tup:\n",
    "            bow_row.append(text.count(word_tup[0]) / word_tup[1])\n",
    "        bow_df.loc[len(bow_df)] = bow_row\n",
    "        \n",
    "    return bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 585.1088128089905s seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Process dataset - Frequency Bag of word\n",
    "n = 10000\n",
    "f_test = frequency_bag_of_word(pre_test, n)  \n",
    "f_valid = frequency_bag_of_word(pre_valid, n)  \n",
    "f_train = frequency_bag_of_word(pre_train, n)\n",
    "\n",
    "f_train = f_train.loc[:, ~f_train.columns.str.contains('^Unnamed')]\n",
    "f_test = f_test.loc[:, ~f_test.columns.str.contains('^Unnamed')]\n",
    "f_valid = f_valid.loc[:, ~f_valid.columns.str.contains('^Unnamed')]\n",
    "\n",
    "print(f\"--- {(time.time() - start_time)}s seconds ---\")\n",
    "\n",
    "# Save bag of words dataframe\n",
    "f_test.to_csv(submit_path + 'f_test.csv')\n",
    "f_valid.to_csv(submit_path + 'f_valid.csv')\n",
    "f_train.to_csv(submit_path + 'f_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bag of words dataframe from file\n",
    "f_test = pd.read_csv(submit_path + 'f_test.csv', sep=\",\")\n",
    "f_valid = pd.read_csv(submit_path + 'f_valid.csv', sep=\",\")\n",
    "f_train = pd.read_csv(submit_path + 'f_train.csv', sep=\",\")\n",
    "\n",
    "f_train = f_train.loc[:, ~f_train.columns.str.contains('^Unnamed')]\n",
    "f_test = f_test.loc[:, ~f_test.columns.str.contains('^Unnamed')]\n",
    "f_valid = f_valid.loc[:, ~f_valid.columns.str.contains('^Unnamed')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_vocab(vocab):\n",
    "    with open('medical_nlp-vocab.txt', 'w') as f:\n",
    "        for i in range(1, len(vocab) + 1):\n",
    "            f.writelines(f\"{vocab[i-1][0]}\\t{i}\\t{vocab[i-1][1]}\\n\")\n",
    "            \n",
    "def submit_set(prefix, df, vocab):\n",
    "    cols = [k for k, v in vocab]\n",
    "    \n",
    "    with open(f'medical {prefix}-traint.txt', 'w') as f:\n",
    "        for index, row in df.iterrows():\n",
    "            for i in range(len(cols)):\n",
    "                if row[i+1] != 0:\n",
    "                    f.write(f'{i+1} ')\n",
    "            f.write(f'\\t{row[0]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "\n",
    "submit_vocab(get_top_n_occ(pre_train, n))\n",
    "submit_set('test', b_test, get_top_n_occ(pre_test, n)) \n",
    "submit_set('valid', b_valid, get_top_n_occ(pre_valid, n)) \n",
    "submit_set('train', b_train, get_top_n_occ(pre_train, n)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 2. Models using binary bag-of-words [19 marks]\n",
    "\n",
    "For this question, we will focus on the Medical-NLP dataset with binary bag-of-words\n",
    "(BBoW) representation. We will use the F1-score as the evaluation metric for the entire\n",
    "assignment. \n",
    "\n",
    "### (a) Random classifier [2 marks]\n",
    "As a baseline, report the performance of the random classifier (a classifier which\n",
    "classifies a review into a uniformly random class) and the majority-class classifier\n",
    "(a classifier which computes the majority class in the training set and classifies all\n",
    "test instances as that majority class). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Random classifier\n",
    "def random_model_f1(df):\n",
    "    n_value = len(df)\n",
    "    possible_value = df['label'].unique()\n",
    "    print(possible_value)\n",
    "    pred_values = [ possible_value[int(np.random.uniform(low=0.0, high=0.9999, size=None) * len(possible_value))] for _ in range(n_value)]\n",
    "    pred_values = np.array(pred_values)\n",
    "    print(pred_values)\n",
    "    real_values = df['label'].to_numpy()\n",
    "    print(real_values)\n",
    "    f1 = f1_score(real_values, pred_values, labels=possible_value, average='macro')\n",
    "    return f1\n",
    "    \n",
    "def most_freq_model_f1(df):\n",
    "    real_values = df['label'].to_numpy()\n",
    "    counts = np.bincount(real_values)\n",
    "    most_freq_value = np.argmax(counts)\n",
    "    pred_values = [most_freq_value] * len(real_values)\n",
    "    f1 = f1_score(real_values, pred_values, labels=df['label'].unique(), average='macro')\n",
    "    return f1\n",
    "\n",
    "def test_model(model, fn, df):\n",
    "    print(f\"f1 {model}_set score: {fn(df)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ~~ Random Model ~~\n",
      "[2 1 3 4]\n",
      "[1 4 2 ... 2 3 3]\n",
      "[2 1 2 ... 1 4 1]\n",
      "f1 train_set score: 0.23985281411732642\n",
      "[2 3 1 4]\n",
      "[4 3 1 2 4 4 3 3 2 4 2 2 1 3 3 3 1 3 1 1 4 1 4 4 2 2 2 3 2 2 3 1 3 4 1 2 4\n",
      " 2 4 3 4 3 4 2 3 1 4 2 1 2 3 1 3 4 4 3 3 3 4 2 2 2 3 3 2 1 2 3 1 3 2 1 3 3\n",
      " 2 4 2 4 4 2 2 4 4 1 4 1 2 4 4 4 2 2 4 3 4 1 3 2 4 4 1 3 1 4 3 3 2 1 1 4 1\n",
      " 1 4 2 2 4 1 1 1 3 1 3 1 3 2 4 3 1 2 2 2 1 3 4 2 3 4 1 2 2 4 1 3 2 2 2 4 3\n",
      " 4 1 3 3 4 3 4 4 3 1 1 1 1 1 4 3 3 3 1 3 2 4 2 3 3 4 1 1 1 3 1 4 1 1 3 2 2\n",
      " 3 1 2 4 4 2 3 4 2 2 2 3 4 4 3 2 4 1 2 4 4 4 2 3 2 4 4 1 3 3 4 2 1 4 3 4 4\n",
      " 4 4 4 1 2 3 2 1 2 3 1 2 1 3 3 2 4 2 1 3 1 2 3 2 4 3 1 2 3 1 3 4 2 4 3 4 1\n",
      " 2 3 2 1 1 2 1 2 2 3 3 1 4 4 2 3 2 2 2 4 1 1 4 2 4 1 4 4 4 3 2 2 3 4 2 1 4\n",
      " 4 2 4 1 4 2 2 4 3 3 4 1 4 3 1 1 2 3 3 4 3 3 3 4 3 2 4 3 2 3 3 3 2 1 4 3 2\n",
      " 3 2 1 1 4 2 3 2 1 3 1 1 2 4 1 3 2 4 1 1 4 2 3 3 2 1 2 4 4 2 3 2 1 2 1 3 4\n",
      " 3 1 1 3 3 4 2 2 4 4 2 3 4 1 1 4 4 1 2 4 1 4 2 2 2 3 4 1 4 1 1 3 2 2 4 1 1\n",
      " 3 1 4 1 1 4 4 2 4 4 1 1 3 2 1 3 3 3 1 3 4 4 4 3 1 1 4 2 3 1 3 3 4 3 1 1 4\n",
      " 2 3 4 3 4 4 4 2 3 4 2 1 1 2 3 3 2 1 4 2 2 1 3 2 4 2 2 2 1 4 4 3 2 2 1 4 2\n",
      " 4 3 3 3 4 3 3 1 4 4 2 4 1 1 2 1 4 2]\n",
      "[2 3 1 2 3 3 2 1 1 4 1 4 1 3 1 1 1 1 1 2 3 2 1 4 2 2 4 1 2 1 2 3 4 2 3 1 4\n",
      " 2 2 4 1 1 1 4 2 3 3 2 4 3 1 1 3 1 1 4 4 1 3 2 2 2 2 1 1 4 2 3 2 3 1 4 3 4\n",
      " 3 3 4 2 3 3 1 2 1 4 1 3 1 1 2 4 2 4 4 2 2 3 2 4 1 3 4 3 2 4 2 4 4 1 1 1 3\n",
      " 1 2 2 1 4 3 4 4 3 4 1 1 3 3 4 2 4 2 3 3 4 2 2 4 2 2 1 1 4 1 1 4 3 4 3 1 1\n",
      " 2 3 3 1 2 1 4 2 2 2 1 4 3 3 1 3 3 4 2 2 2 1 1 3 3 4 3 1 1 2 1 4 1 1 1 4 1\n",
      " 3 3 3 3 4 3 1 2 2 2 4 4 3 3 1 1 4 1 2 1 2 4 1 1 1 1 2 2 3 1 1 1 1 4 3 4 3\n",
      " 3 1 2 4 1 3 4 1 1 3 2 1 4 1 2 2 1 2 3 4 4 4 4 2 3 2 2 3 3 1 3 3 3 2 3 4 4\n",
      " 1 4 4 2 4 4 2 1 3 2 2 1 2 1 4 1 1 1 2 1 1 1 4 1 2 1 3 4 1 3 3 1 2 3 1 1 3\n",
      " 2 1 3 1 4 1 1 4 3 2 4 3 2 1 1 4 3 2 1 1 4 2 3 3 1 3 2 1 2 4 2 1 1 3 2 2 1\n",
      " 2 2 3 3 3 2 3 1 3 2 2 3 2 1 4 2 4 3 2 3 2 2 3 1 1 1 1 4 4 1 2 1 2 3 1 3 4\n",
      " 4 1 3 4 4 3 1 2 2 4 2 4 1 1 2 4 1 4 1 4 3 2 4 4 1 3 2 4 1 3 1 3 1 4 4 3 4\n",
      " 3 2 1 1 1 1 2 1 1 1 4 1 1 2 3 2 1 1 3 3 4 4 1 2 1 3 3 1 1 1 3 2 2 1 2 1 3\n",
      " 1 1 1 2 4 1 1 4 1 4 1 1 4 4 3 4 1 4 4 4 1 4 2 1 4 2 2 1 2 1 2 1 3 3 4 2 1\n",
      " 1 2 1 4 4 3 2 4 1 4 4 1 1 2 2 4 1 2]\n",
      "f1 valid_set score: 0.2927438651868445\n",
      "[2 1 4 3]\n",
      "[3 3 1 1 3 1 4 4 2 2 2 2 2 2 4 3 1 3 4 1 2 4 1 4 2 1 4 3 2 1 2 1 3 3 4 2 3\n",
      " 4 4 1 1 3 3 2 4 2 3 4 3 2 4 3 4 2 1 1 3 3 4 4 1 2 2 2 4 4 1 2 3 2 2 3 1 1\n",
      " 1 3 4 4 4 1 1 2 1 4 4 2 2 4 1 4 4 4 3 4 2 3 2 4 2 1 2 1 4 4 1 1 4 4 3 1 3\n",
      " 4 1 4 1 3 2 2 4 3 4 4 1 4 2 4 1 1 4 2 4 1 3 2 1 3 2 2 1 4 2 3 4 2 3 2 3 1\n",
      " 1 4 1 3 4 1 2 1 4 2 4 2 4 3 4 2 1 3 2 2 4 3 4 3 1 4 1 4 2 3 1 1 3 4 3 2 1\n",
      " 4 2 1 4 4 2 3 3 3 4 4 1 4 1 1 2 1 3 3 3 4 4 1 3 2 2 4 1 1 1 2 3 1 3 2 2 3\n",
      " 3 1 1 4 4 1 1 1 3 4 1 1 2 1 4 4 3 1 2 1 2 4 3 2 3 3 2 4 3 4 1 3 3 4 1 2 2\n",
      " 2 3 2 4 1 4 4 1 1 1 2 1 3 3 1 1 3 2 2 1 4 1 1 4 1 4 4 4 2 1 3 1 3 1 2 4 2\n",
      " 2 4 2 2 3 4 1 4 1 3 2 4 1 4 2 3 2 2 3 4 3 1 2 4 4 1 2 2 2 4 1 3 1 2 3 4 2\n",
      " 1 2 1 2 2 2 2 3 2 3 4 3 4 4 3 4 4 2 3 3 3 4 1 4 4 2 3 2 4 2 1 3 1 4 1 1 4\n",
      " 1 1 3 2 3 2 2 4 1 1 1 2 2 3 1 3 3 1 3 4 4 2 2 1 3 1 3 4 3 2 1 1 1 2 3 4 2\n",
      " 1 2 4 3 4 2 4 2 1 4 4 2 3 1 1 1 3 3 1 3 3 2 1 1 3 3 1 4 3 1 3 1 3 2 2 3 4\n",
      " 3 3 4 4 1 3 3 3 3 2 2 2 1 2 3 3 4 3 1 1 3 1 2 3 2 2 2 2 2 3 4 2 3 2 2 3 3\n",
      " 3 4 2 4 4 4 2 2 1 1 4 4 3 2 1 2 3 3 4]\n",
      "[2 1 2 4 1 4 4 3 3 1 2 2 1 1 1 1 1 1 1 3 1 4 1 4 1 3 1 2 1 1 3 1 4 2 3 3 4\n",
      " 1 1 4 4 2 1 1 1 1 4 2 3 2 3 3 2 2 3 1 2 2 1 1 1 1 1 2 2 3 2 1 4 1 4 2 1 4\n",
      " 1 2 3 4 1 4 4 1 4 4 1 1 3 2 1 3 3 4 3 4 1 4 3 1 1 4 3 2 2 4 2 1 4 1 4 3 3\n",
      " 3 1 3 1 1 2 2 1 2 2 2 3 2 1 3 3 4 1 1 3 1 2 1 2 1 2 1 2 3 4 2 2 1 3 3 4 4\n",
      " 3 1 3 4 1 3 4 3 1 1 1 3 1 3 1 1 1 1 2 2 1 1 2 4 1 1 1 2 4 3 1 3 4 4 2 4 2\n",
      " 1 1 2 3 4 4 3 1 1 1 2 1 4 1 1 2 1 1 2 4 2 2 1 3 2 1 1 2 3 1 2 4 1 3 4 2 3\n",
      " 1 1 2 1 3 4 4 1 3 1 2 1 1 1 3 3 1 2 1 1 3 2 1 2 1 2 2 2 2 2 3 2 1 3 3 3 1\n",
      " 3 1 4 1 1 2 2 1 3 2 3 3 1 3 1 2 1 4 1 3 4 4 4 4 1 1 1 1 4 4 4 1 4 2 1 1 1\n",
      " 1 2 1 3 3 3 4 2 3 1 1 2 1 2 1 4 4 1 3 3 4 1 3 1 3 2 2 3 4 1 2 1 4 4 4 3 2\n",
      " 1 2 4 1 3 2 3 2 3 3 3 2 1 2 4 4 2 1 1 3 4 4 1 1 3 3 1 3 3 1 4 1 3 3 3 4 1\n",
      " 3 2 3 1 4 3 1 3 2 3 3 3 3 1 3 1 3 1 1 1 2 2 1 3 1 1 2 2 3 4 4 1 2 1 4 3 4\n",
      " 1 1 1 4 2 1 4 4 1 1 1 1 2 1 2 1 3 2 1 3 1 3 1 1 3 3 1 1 1 1 4 3 1 1 2 1 1\n",
      " 4 1 1 1 1 4 2 1 3 1 1 2 4 3 4 2 3 1 3 2 4 1 1 2 3 4 4 3 1 1 4 1 1 2 4 2 1\n",
      " 3 2 2 4 3 4 1 1 1 1 1 4 1 4 1 1 1 2 1]\n",
      "f1 test_set score: 0.2443561931799712\n",
      "\n",
      "\n",
      " ~~ Most Frequence Model ~~\n",
      "f1 train_set score: 0.120996778472617\n",
      "f1 valid_set score: 0.12424698795180723\n",
      "f1 test_set score: 0.14183381088825217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\" ~~ Random Model ~~\")\n",
    "test_model('train', random_model_f1, b_train)\n",
    "test_model('valid', random_model_f1, b_valid)\n",
    "test_model('test', random_model_f1, b_test)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\" ~~ Most Frequence Model ~~\")\n",
    "test_model('train', most_freq_model_f1, b_train)\n",
    "test_model('valid', most_freq_model_f1, b_valid)\n",
    "test_model('test', most_freq_model_f1, b_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21.3\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Naive Bayes, Decision Trees, Logistic regression and Linear SVM [8 marks]\n",
    "Now train Naive Bayes, Decision Trees, Logistic regression and Linear SVM for this\n",
    "task. \n",
    "\n",
    "[Note: You should do a thorough hyper-parameter tuning by using the given\n",
    "validation set. Also, note that you should use the appropriate naive Bayes classifier\n",
    "for binary input features (also called Bernoulli naive Bayes).] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(df):\n",
    "    X = df.drop(columns=['label']).to_numpy()\n",
    "    y = df['label'].to_numpy()\n",
    "    return X, y\n",
    "\n",
    "def f1_bench(clf, df, print_=True):\n",
    "    # Prediction\n",
    "    X, real_values = extract_data(df)\n",
    "    pred_values = clf.predict(X)\n",
    "    \n",
    "    # f1 score\n",
    "    f1 = f1_score(real_values, pred_values, labels=df['label'].unique(), average='macro')\n",
    "    if print_:\n",
    "        print(f\"f1 score: {f1}\")\n",
    "    else:\n",
    "        return f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X, y = extract_data(b_test)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 0.5225228733709496\n",
      "valid : 0.19589150434516062\n",
      "test : 0.18533185585758225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "def bernoulli_naive_bayes(df, **kwargs):\n",
    "    X, y = extract_data(df)\n",
    "    return BernoulliNB(**kwargs).fit(X, y)\n",
    "\n",
    "clf = bernoulli_naive_bayes(b_train)\n",
    "\n",
    "print(f\"train : {f1_bench(clf, b_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, b_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, b_test, False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 0.8172281157316901\n",
      "valid : 0.340993471532188\n",
      "test : 0.33200323263278014\n"
     ]
    }
   ],
   "source": [
    "def decision_tree(df, **kwargs):\n",
    "    X, y = extract_data(df)\n",
    "    return DecisionTreeClassifier(**kwargs).fit(X, y)\n",
    "\n",
    "clf = decision_tree(b_train)\n",
    "\n",
    "print(f\"train : {f1_bench(clf, b_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, b_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, b_test, False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 0.8254875067208343\n",
      "valid : 0.25581763529845875\n",
      "test : 0.256838068595216\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(df, **kwargs):\n",
    "    X, y = extract_data(df)\n",
    "    return LogisticRegression(**kwargs).fit(X, y)\n",
    "\n",
    "clf = logistic_regression(b_train)\n",
    "\n",
    "print(f\"train : {f1_bench(clf, b_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, b_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, b_test, False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-db9280f34c10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"train : {f1_bench(clf, b_train, False)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"valid : {f1_bench(clf, b_valid, False)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"test : {f1_bench(clf, b_test, False)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-849b8cabd4fd>\u001b[0m in \u001b[0;36mf1_bench\u001b[0;34m(clf, df, print_)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpred_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# f1 score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_final_estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \"\"\"\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dense_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobA_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobB_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvm_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mdegree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             cache_size=self.cache_size)\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sparse_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def SVM(df, **kwargs):\n",
    "    X, y = extract_data(df)\n",
    "    clf = make_pipeline(StandardScaler(), SVC(**kwargs))\n",
    "    clf.fit(X, y)\n",
    "    return clf\n",
    "\n",
    "clf = SVM(b_train)\n",
    "\n",
    "print(f\"train : {f1_bench(clf, b_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, b_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, b_test, False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Report the list of hyper-parameters [3 marks]\n",
    "Report the list of hyper-parameters you considered for each classifier, their range,\n",
    "as well as the best values for these hyper-parameters, chosen based on the validation\n",
    "set performance1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d302006cf4a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "X, y = extract_data(b_train)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  18 | elapsed:   10.5s remaining:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:   12.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1}\n",
      "Naive Bayes\n",
      "train : 0.5171964299159018\n",
      "valid : 0.10999999999999999\n",
      "test : 0.11841285063533924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Naive bayes\n",
    "X, y = extract_data(f_train)\n",
    "param_DT_bbow = {\n",
    "    'alpha': [1, 0.1, 0.01, 0.001, 1.0e-6, 1.0e-10],\n",
    "}\n",
    "\n",
    "DT_grid_search = GridSearchCV(estimator=BernoulliNB(), param_grid=param_DT_bbow, refit=False, verbose=3, n_jobs=-1)\n",
    "DT_grid_search.fit(X, y)\n",
    "\n",
    "print(DT_grid_search.best_params_)\n",
    "\n",
    "clf = bernoulli_naive_bayes(f_train, **DT_grid_search.best_params_)\n",
    "print(\"Naive Bayes\")\n",
    "print(f\"train : {f1_bench(clf, f_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, f_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, f_test, False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] criterion=gini, max_depth=None, min_samples_split=2, splitter=best \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_split=2, splitter=best, score=0.688, total=   5.5s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=2, splitter=best \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    5.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=gini, max_depth=None, min_samples_split=2, splitter=best, score=0.692, total=   4.9s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=2, splitter=best \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   10.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=gini, max_depth=None, min_samples_split=2, splitter=best, score=0.692, total=   5.1s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=2, splitter=random \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_split=2, splitter=random, score=0.699, total=   4.8s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=2, splitter=random \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_split=2, splitter=random, score=0.695, total=   4.7s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=2, splitter=random \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_split=2, splitter=random, score=0.696, total=   4.8s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=4, splitter=best \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_split=4, splitter=best, score=0.691, total=   5.5s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=4, splitter=best \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_split=4, splitter=best, score=0.682, total=   5.2s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=4, splitter=best \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_split=4, splitter=best, score=0.686, total=   5.1s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=4, splitter=random \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_split=4, splitter=random, score=0.718, total=   4.6s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=4, splitter=random \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_split=4, splitter=random, score=0.695, total=   4.4s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=4, splitter=random \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_split=4, splitter=random, score=0.682, total=   4.6s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=10, splitter=best \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_split=10, splitter=best, score=0.704, total=   5.1s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=10, splitter=best \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_split=10, splitter=best, score=0.701, total=   4.8s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=10, splitter=best \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_split=10, splitter=best, score=0.698, total=   5.1s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=10, splitter=random \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_split=10, splitter=random, score=0.697, total=   4.4s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=10, splitter=random \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_split=10, splitter=random, score=0.691, total=   4.9s\n",
      "[CV] criterion=gini, max_depth=None, min_samples_split=10, splitter=random \n",
      "[CV]  criterion=gini, max_depth=None, min_samples_split=10, splitter=random, score=0.679, total=   4.6s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=2, splitter=best \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=2, splitter=best, score=0.694, total=   5.2s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=2, splitter=best \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=2, splitter=best, score=0.695, total=   5.2s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=2, splitter=best \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=2, splitter=best, score=0.686, total=   5.3s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=2, splitter=random \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=2, splitter=random, score=0.699, total=   4.4s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=2, splitter=random \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=2, splitter=random, score=0.695, total=   4.6s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=2, splitter=random \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=2, splitter=random, score=0.686, total=   4.6s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=4, splitter=best \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=4, splitter=best, score=0.692, total=   5.5s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=4, splitter=best \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=4, splitter=best, score=0.694, total=   5.3s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=4, splitter=best \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=4, splitter=best, score=0.683, total=   6.3s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=4, splitter=random \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=4, splitter=random, score=0.692, total=   5.5s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=4, splitter=random \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=4, splitter=random, score=0.716, total=   5.5s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=4, splitter=random \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=4, splitter=random, score=0.688, total=   4.7s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=10, splitter=best \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=10, splitter=best, score=0.704, total=   5.9s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=10, splitter=best \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=10, splitter=best, score=0.692, total=   5.8s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=10, splitter=best \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=10, splitter=best, score=0.696, total=   4.9s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=10, splitter=random \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=10, splitter=random, score=0.717, total=   4.3s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=10, splitter=random \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=10, splitter=random, score=0.710, total=   4.5s\n",
      "[CV] criterion=gini, max_depth=4000, min_samples_split=10, splitter=random \n",
      "[CV]  criterion=gini, max_depth=4000, min_samples_split=10, splitter=random, score=0.691, total=   4.4s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=2, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=2, splitter=best, score=0.669, total=   4.6s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=2, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=2, splitter=best, score=0.675, total=   4.3s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=2, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=2, splitter=best, score=0.680, total=   4.2s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=2, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=2, splitter=random, score=0.715, total=   4.0s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=2, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=2, splitter=random, score=0.668, total=   4.0s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=2, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=2, splitter=random, score=0.696, total=   4.1s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=4, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=4, splitter=best, score=0.674, total=   4.5s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=4, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=4, splitter=best, score=0.674, total=   4.4s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=4, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=4, splitter=best, score=0.682, total=   4.9s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=4, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=4, splitter=random, score=0.678, total=   4.1s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=4, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=4, splitter=random, score=0.698, total=   4.1s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=4, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=4, splitter=random, score=0.682, total=   3.9s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=10, splitter=best \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=10, splitter=best, score=0.677, total=   4.4s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=10, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=10, splitter=best, score=0.677, total=   4.2s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=10, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=10, splitter=best, score=0.688, total=   4.1s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=10, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=10, splitter=random, score=0.708, total=   4.2s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=10, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=10, splitter=random, score=0.736, total=   3.9s\n",
      "[CV] criterion=entropy, max_depth=None, min_samples_split=10, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=None, min_samples_split=10, splitter=random, score=0.692, total=   3.5s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=2, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=2, splitter=best, score=0.667, total=   4.5s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=2, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=2, splitter=best, score=0.680, total=   4.3s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=2, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=2, splitter=best, score=0.683, total=   4.2s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=2, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=2, splitter=random, score=0.674, total=   3.9s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=2, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=2, splitter=random, score=0.670, total=   4.2s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=2, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=2, splitter=random, score=0.693, total=   4.0s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=4, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=4, splitter=best, score=0.660, total=   4.5s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=4, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=4, splitter=best, score=0.672, total=   4.3s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=4, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=4, splitter=best, score=0.682, total=   4.2s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=4, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=4, splitter=random, score=0.669, total=   3.7s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=4, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=4, splitter=random, score=0.694, total=   4.1s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=4, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=4, splitter=random, score=0.696, total=   3.9s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=10, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=10, splitter=best, score=0.664, total=   4.4s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=10, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=10, splitter=best, score=0.671, total=   4.1s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=10, splitter=best \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=10, splitter=best, score=0.685, total=   4.1s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=10, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=10, splitter=random, score=0.685, total=   4.0s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=10, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=10, splitter=random, score=0.692, total=   3.8s\n",
      "[CV] criterion=entropy, max_depth=4000, min_samples_split=10, splitter=random \n",
      "[CV]  criterion=entropy, max_depth=4000, min_samples_split=10, splitter=random, score=0.677, total=   3.8s\n",
      "{'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'splitter': 'random'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree\n",
      "train : 0.7899652463968564\n",
      "valid : 0.29670788763116773\n",
      "test : 0.30538861473283374\n"
     ]
    }
   ],
   "source": [
    "X, y = extract_data(f_train)\n",
    "param_DT_bbow = {\n",
    "    'criterion': [\"gini\", \"entropy\"],\n",
    "    'splitter': [\"best\", \"random\"],\n",
    "    'max_depth': [None, 4000],\n",
    "    'min_samples_split': [2, 4, 10]\n",
    "}\n",
    "\n",
    "DT_grid_search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=param_DT_bbow, refit=False, verbose=3)\n",
    "DT_grid_search.fit(X, y)\n",
    "\n",
    "print(DT_grid_search.best_params_)\n",
    "\n",
    "clf = decision_tree(b_train, **DT_grid_search.best_params_)\n",
    "print(\"Decision tree\")\n",
    "print(f\"train : {f1_bench(clf, b_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, b_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, b_test, False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    }
   ],
   "source": [
    "X, y = extract_data(f_train)\n",
    "# param_DT_bbow = {\n",
    "#     'solver': ['newton-cg'], \n",
    "#     'penalty': ['l2', 'none'], \n",
    "#     'C': [1.0, 0.1], \n",
    "#     'dual': [False],\n",
    "#     'multi_class': ['multinomial']\n",
    "# }\n",
    "\n",
    "# DT_grid_search = GridSearchCV(estimator=LogisticRegression(), param_grid=param_DT_bbow, refit=False, verbose=3)\n",
    "# DT_grid_search.fit(X, y)\n",
    "\n",
    "# print(DT_grid_search.best_params_)\n",
    "\n",
    "# clf = logistic_regression(b_train, **DT_grid_search.best_params_)\n",
    "# print(\"Decision tree\")\n",
    "# print(f\"train : {f1_bench(clf, b_train, False)}\")\n",
    "# print(f\"valid : {f1_bench(clf, b_valid, False)}\")\n",
    "# print(f\"test : {f1_bench(clf, b_test, False)}\")\n",
    "\n",
    "param_DT_bbow = {\n",
    "#     'solver': ['lbfgs'], \n",
    "#     'penalty': ['l2', 'elasticnet', 'none'], \n",
    "#     'C': [1.0, 0.1], \n",
    "#     'dual': [False],\n",
    "#     'multi_class': ['multinomial']\n",
    "    'C':0.1, 'multi_class':'multinomial', 'penalty':'none', 'solver':'newton-cg'\n",
    "}\n",
    "\n",
    "clf = logistic_regression(b_train, **param_DT_bbow)\n",
    "print(\"Decision tree\")\n",
    "print(f\"train : {f1_bench(clf, b_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, b_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, b_test, False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.2560252834861072\n"
     ]
    }
   ],
   "source": [
    "hyper_params = {\n",
    "    'gamma':'auto',\n",
    "    'kernel': 'sigmoid',\n",
    "    'degree': 5,\n",
    "    'coef0': 1.0,\n",
    "    'shrinking': True,\n",
    "    'probability': False,\n",
    "    'tol': 1e-3,\n",
    "    'cache_size': 200,\n",
    "    'max_iter': -1,\n",
    "    'random_state': None\n",
    "}\n",
    "clf = SVM(b_train, **hyper_params)\n",
    "\n",
    "print(\"SVM\")\n",
    "print(f\"train : {f1_bench(clf, b_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, b_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, b_test, False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) F1-score [3 marks]\n",
    "Report the training, validation, and test F1-score for all the classifiers  (with best hyper-parameter configuration). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "train : 0.5761029784481652\n",
      "valid : 0.21121649184149183\n",
      "test : 0.20312014064619555\n",
      "Decision Tree\n",
      "train : 0.6956109917518675\n",
      "valid : 0.3613305487071276\n",
      "test : 0.26362123164636053\n",
      "Log Reg\n",
      "train : 0.8307231436953693\n",
      "valid : 0.262772782271148\n",
      "test : 0.25139234113604436\n",
      "SVM\n",
      "train : 0.5018864077080809\n",
      "valid : 0.2560252834861072\n",
      "test : 0.28256980681798927\n"
     ]
    }
   ],
   "source": [
    "# Naive bayes\n",
    "hyper_params = {\n",
    "    'alpha': 1.0e-10,\n",
    "    'binarize': 0.0,\n",
    "    'fit_prior': True\n",
    "}\n",
    "clf = bernoulli_naive_bayes(b_train, **hyper_params)\n",
    "\n",
    "print(\"Naive Bayes\")\n",
    "print(f\"train : {f1_bench(clf, b_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, b_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, b_test, False)}\")\n",
    "\n",
    "hyper_params = {\n",
    "    'criterion':\"gini\",\n",
    "    'splitter':\"best\", \n",
    "    'max_depth': 4000,\n",
    "    'min_samples_split': 2, \n",
    "    'min_samples_leaf' : 10,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'max_features': None,\n",
    "    'random_state': 1000,\n",
    "    'max_leaf_nodes': 100,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'class_weight': None\n",
    "}\n",
    "clf = decision_tree(b_train, **hyper_params)\n",
    "\n",
    "print(\"\\nDecision Tree\")\n",
    "print(f\"train : {f1_bench(clf, b_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, b_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, b_test, False)}\")\n",
    "\n",
    "hyper_params = { \n",
    "    'solver':'newton-cg', \n",
    "    'penalty': 'l2', \n",
    "    'C':1.0, \n",
    "    'dual': False,\n",
    "    'tol': 1e-4,\n",
    "    'fit_intercept': True,\n",
    "    'random_state': None,\n",
    "    'intercept_scaling': 1,\n",
    "    'max_iter':100, \n",
    "    'multi_class':'multinomial'\n",
    "}\n",
    "clf = logistic_regression(b_train, **hyper_params)\n",
    "\n",
    "print(\"\\nLog Reg\")\n",
    "print(f\"train : {f1_bench(clf, b_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, b_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, b_test, False)}\")\n",
    "\n",
    "hyper_params = {\n",
    "    'gamma':'auto',\n",
    "    'kernel': 'sigmoid',\n",
    "    'degree': 5,\n",
    "    'coef0': 1.0,\n",
    "    'shrinking': True,\n",
    "    'probability': False,\n",
    "    'tol': 1e-3,\n",
    "    'cache_size': 200,\n",
    "    'max_iter': -1,\n",
    "    'random_state': None\n",
    "}\n",
    "clf = SVM(b_train, **hyper_params)\n",
    "\n",
    "print(\"\\nSVM\")\n",
    "print(f\"train : {f1_bench(clf, b_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, b_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, b_test, False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Comment on the performance of different classifiers. [3 marks]\n",
    "\n",
    "Why did a particular classifier\n",
    "perform better than the rest? What was the role of the hyper-parameters in finding\n",
    "the best results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model using frequency bag-of-words [21 marks]\n",
    "\n",
    "Now we will repeat question 2 but with frequency bag-of-words (FBoW) representation.\n",
    "\n",
    "### (a) Naive Bayes, Decision Tree, Logistic regression and Linear SVM  [8 marks]\n",
    "Train Naive Bayes, Decision Tree, Logistic regression and Linear SVM for this task.\n",
    "\n",
    "[Note: Again, you should do a thorough hyper-parameter tuning by using the given\n",
    "validation set. Also, note that you should use the appropriate naive Bayes classifier\n",
    "for real valued input features (also called Gaussian naive Bayes).] [8 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def gaussian_naive_bayes(df, **kwargs):\n",
    "    X, y = extract_data(df)\n",
    "    return GaussianNB(**kwargs).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naivie bayes\n",
      "train : 0.5171964299159018\n",
      "valid : 0.10999999999999999\n",
      "test : 0.11841285063533924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "train : 1.0\n",
      "valid : 0.21554428491806374\n",
      "test : 0.17420969070172534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Reg\n",
      "train : 0.6488389753831039\n",
      "valid : 0.1757513807786575\n",
      "test : 0.07911457265833162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "train : 0.6669423820274355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid : 0.090311986863711\n",
      "test : 0.07698815566835872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = bernoulli_naive_bayes(f_train)\n",
    "print(\"Naivie bayes\")\n",
    "print(f\"train : {f1_bench(clf, f_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, f_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, f_test, False)}\")\n",
    "\n",
    "clf = decision_tree(f_train)\n",
    "print(\"Decision Tree\")\n",
    "print(f\"train : {f1_bench(clf, f_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, f_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, f_test, False)}\")\n",
    "\n",
    "clf = logistic_regression(f_train)\n",
    "print(\"Log Reg\")\n",
    "print(f\"train : {f1_bench(clf, f_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, f_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, f_test, False)}\")\n",
    "\n",
    "clf = SVM(f_train)\n",
    "print(\"SVM\")\n",
    "print(f\"train : {f1_bench(clf, f_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, f_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, f_test, False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Report the list of hyper-parameters [3 marks]\n",
    "Report the list of hyper-parameters you considered for each classifier, their range,\n",
    "as well as the best values for these hyper-parameters, chosen based on the validation\n",
    "set performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   36.3s\n",
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 576 out of 576 | elapsed: 11.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 10, 'min_samples_split': 2, 'random_state': None, 'splitter': 'random'}\n",
      "train : 0.8129893829433428\n",
      "valid : 0.1966441340149671\n",
      "test : 0.20046829222817086\n"
     ]
    }
   ],
   "source": [
    "X, y = extract_data(f_train)\n",
    "param_DT_bbow = {\n",
    "    'criterion': [\"gini\", 'entropy'],\n",
    "    'splitter': [\"best\", \"random\"],\n",
    "    'max_depth': [None, 1000, 4000],\n",
    "    'min_samples_split': [2, 10], \n",
    "    'min_samples_leaf' : [1, 10],\n",
    "    'max_features': [None, 1000],\n",
    "    'random_state': [None, 1000]\n",
    "}\n",
    "\n",
    "DT_grid_search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=param_DT_bbow, refit=False, verbose=3, n_jobs=-1)\n",
    "DT_grid_search.fit(X, y)\n",
    "\n",
    "print(DT_grid_search.best_params_)\n",
    "clf = decision_tree(f_train, **DT_grid_search.best_params_)\n",
    "print(f\"train : {f1_bench(clf, f_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, f_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, f_test, False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_visualization(X_train_clf,y_train_clf, DT_grid_search,'Logistic Regression-Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = f_train\n",
    "valid = f_valid\n",
    "test = f_test\n",
    "\n",
    "# Naive bayes\n",
    "hyper_params = {\n",
    "    'alpha': 1.0e-10,\n",
    "    'binarize': 0.0,\n",
    "    'fit_prior': True\n",
    "}\n",
    "clf = bernoulli_naive_bayes(f_train, **hyper_params)\n",
    "\n",
    "print(\"Naive Bayes\")\n",
    "print(f\"train : {f1_bench(clf, b_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, b_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, b_test, False)}\")\n",
    "\n",
    "hyper_params = {\n",
    "    'criterion':\"gini\",\n",
    "    'splitter':\"best\", \n",
    "    'max_depth': 4000,\n",
    "    'min_samples_split': 2, \n",
    "    'min_samples_leaf' : 10,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'max_features': None,\n",
    "    'random_state': 1000,\n",
    "    'max_leaf_nodes': 100,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'class_weight': None\n",
    "}\n",
    "clf = decision_tree(b_train, **hyper_params)\n",
    "\n",
    "print(\"\\nDecision Tree\")\n",
    "print(f\"train : {f1_bench(clf, b_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, b_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, b_test, False)}\")\n",
    "\n",
    "hyper_params = { \n",
    "    'solver':'newton-cg', \n",
    "    'penalty': 'l2', \n",
    "    'C':1.0, \n",
    "    'dual': False,\n",
    "    'tol': 1e-4,\n",
    "    'fit_intercept': True,\n",
    "    'random_state': None,\n",
    "    'intercept_scaling': 1,\n",
    "    'max_iter':100, \n",
    "    'multi_class':'multinomial'\n",
    "}\n",
    "clf = logistic_regression(b_train, **hyper_params)\n",
    "\n",
    "print(\"\\nLog Reg\")\n",
    "print(f\"train : {f1_bench(clf, b_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, b_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, b_test, False)}\")\n",
    "\n",
    "hyper_params = {\n",
    "    'gamma':'auto',\n",
    "    'kernel': 'sigmoid',\n",
    "    'degree': 5,\n",
    "    'coef0': 1.0,\n",
    "    'shrinking': True,\n",
    "    'probability': False,\n",
    "    'tol': 1e-3,\n",
    "    'cache_size': 200,\n",
    "    'max_iter': -1,\n",
    "    'random_state': None\n",
    "}\n",
    "clf = SVM(b_train, **hyper_params)\n",
    "\n",
    "print(\"\\nSVM\")\n",
    "print(f\"train : {f1_bench(clf, b_train, False)}\")\n",
    "print(f\"valid : {f1_bench(clf, b_valid, False)}\")\n",
    "print(f\"test : {f1_bench(clf, b_test, False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) F1-score [3 marks]\n",
    "Report the training, validation, and test F1-score for all the classifiers (with best\n",
    "hyper-parameter configuration). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Comment on the performance of different classifiers [3 mark]\n",
    "Why did a particular classifier perform better than the rest? What was the role of the hyper-parameters in finding\n",
    "the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Compare the performance with the binary bag-of-words based classifiers. [2 mark]\n",
    "\n",
    "Why is\n",
    "there a difference in the performance? Give a brief explanation comparing BBoW\n",
    "Naive Bayes and FBoW Naive Bayes and similarly for other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f)  Comment on the representation [2 mark]\n",
    "Which representation is better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(f\"--- {(time.time() - start_time)}s seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>the</th>\n",
       "      <th>and</th>\n",
       "      <th>was</th>\n",
       "      <th>of</th>\n",
       "      <th>to</th>\n",
       "      <th>a</th>\n",
       "      <th>with</th>\n",
       "      <th>in</th>\n",
       "      <th>...</th>\n",
       "      <th>arthritis4</th>\n",
       "      <th>junctions</th>\n",
       "      <th>amoxil</th>\n",
       "      <th>prevented</th>\n",
       "      <th>3000</th>\n",
       "      <th>hypertensionallergies</th>\n",
       "      <th>approval</th>\n",
       "      <th>25mm</th>\n",
       "      <th>tapers</th>\n",
       "      <th>c5c6postoperative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.004675</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.007176</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label       the       and       was        of        to  \\\n",
       "0           0    2.0  0.000017  0.000030  0.000000  0.000021  0.000074   \n",
       "1           1    1.0  0.000342  0.000135  0.000285  0.000186  0.000394   \n",
       "2           2    2.0  0.000094  0.000255  0.000036  0.000145  0.000296   \n",
       "3           3    1.0  0.000461  0.000359  0.000374  0.000290  0.000542   \n",
       "4           4    2.0  0.000120  0.000195  0.000125  0.000414  0.000517   \n",
       "\n",
       "          a      with        in  ...  arthritis4  junctions  amoxil  \\\n",
       "0  0.001280  0.000035  0.000192  ...         0.0        0.0     0.0   \n",
       "1  0.004675  0.000211  0.000882  ...         0.0        0.0     0.0   \n",
       "2  0.005776  0.000141  0.001764  ...         0.0        0.0     0.0   \n",
       "3  0.007176  0.000423  0.002493  ...         0.0        0.0     0.0   \n",
       "4  0.004883  0.000388  0.000997  ...         0.0        0.0     0.0   \n",
       "\n",
       "   prevented  3000  hypertensionallergies  approval  25mm  tapers  \\\n",
       "0        0.0   0.0                    0.0       0.0   0.0     0.0   \n",
       "1        0.0   0.0                    0.0       0.0   0.0     0.0   \n",
       "2        0.0   0.0                    0.0       0.0   0.0     0.0   \n",
       "3        0.0   0.0                    0.0       0.0   0.0     0.0   \n",
       "4        0.0   0.0                    0.0       0.0   0.0     0.0   \n",
       "\n",
       "   c5c6postoperative  \n",
       "0                0.0  \n",
       "1                0.0  \n",
       "2                0.0  \n",
       "3                0.0  \n",
       "4                0.0  \n",
       "\n",
       "[5 rows x 10002 columns]"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   23.5s\n",
      "/Users/guillaumethibault/opt/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 324 out of 324 | elapsed:  9.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': None, 'max_leaf_nodes': 100, 'min_samples_leaf': 100, 'min_samples_split': 2, 'random_state': 10}\n",
      "f1 score: 0.32989399205709746\n"
     ]
    }
   ],
   "source": [
    "param_DT_bbow = {\n",
    "    'max_depth': [None, 100, 4000],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf' : [10, 100],\n",
    "    'random_state': [10, 100, 1000],\n",
    "    'max_leaf_nodes': [10, 100, 1000]\n",
    "}\n",
    "\n",
    "DT_grid_search = GridSearchCV(estimator=DecisionTreeClassifier(), param_grid=param_DT_bbow, refit=False, verbose=3, n_jobs=-1)\n",
    "DT_grid_search.fit(X, y)\n",
    "\n",
    "print(DT_grid_search.best_params_)\n",
    "clf = decision_tree(b_train, **DT_grid_search.best_params_)\n",
    "f1_bench(clf, b_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dd864889f000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "print(sum(a))\n",
    "a = a / sum(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
